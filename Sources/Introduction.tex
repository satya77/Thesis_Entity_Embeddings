%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{chap:intro}
The long-standing challenge for research in computer science is the understanding of written text and extraction of useful information from it. Text that is created by humans is often unstructured and ambiguous, whereas machine learning algorithms prefer well-defined fixed-length inputs and outputs. Therefore, machine learning algorithms cannot work with raw text directly and the text has to be converted into vectors of features.
\\
\noindent
Traditional \emph{Natural Language Processing} (NLP) systems treat words as atomic units. Regarding vectors, each word would be treated as a one-hot vector the size of the vocabulary. These vectors are sparse and orthogonal, and therefore contain no notion of similarity among themselves. For example, if a user is searching for \emph{``Heidelberg Hotel''}, documents containing \emph{``Heidelberg Motel''} will be disregard since the dot product of the one-hot vectors of \emph{``Hotel''} and \emph{``Motel''} is zero. On the other hand, distributed vector space models represent words in a continuous vector space, where a word is represented by its neighbors' means. These models are based on the idea that similar words tend to happen in a similar context or as the British linguist, J.R. Firth, said:
\\
\\
\noindent
\say{You shall know a word by the company it keeps.} (J.R. Firth 1957)\\
\\
\noindent
These distributed representations try to map words to a dense vector, such that words with closer meanings are mapped to the nearby points and the similarity between them can be computed based on their distance in space.
\\
\noindent 
Learning dense vector representations of words was introduced by the pioneering paper by~\brackettext{\cite{Bengio:2003:NPL:944919.944966}}, in which a feature vector, much smaller than the size of the vocabulary, was used to express words using a probabilistic model. The work of~\brackettext{\cite{Collobert:2011:NLP:1953048.2078186}} proved that word representation could not only be achieved through probabilistic models, but that neural network architectures can also learn these internal representations from vast amounts of mostly unlabeled training data.
\\
\noindent
The first simple and scalable model to learn such embeddings is \emph{word2vec} presented by \brackettext{\cite{DBLP:journals/corr/abs-1301-3781}}, where the aim is to capture the meaning of a word based on the surrounding context. The goal of the model is, given a center word, to predict the words that occur in its surroundings. word2vec uses a shallow neural network with one hidden layer to process non-labeled documents. The neural network architecture contains two models, \emph{continuous bag-of-words} (CBOW) and the \emph{skip-gram} architecture. Both are shown in Figure~\ref{fig:skip_gram}. The CBOW architecture predicts the current word based on the context and the skip-gram predicts surrounding words given the current word.
\begin{figure}
\centering

\includegraphics[width=0.7\linewidth , height=0.5\linewidth]{images/CBOW_SkipGram.png}
\caption{Predict a word, given the preceding and following words (Continuous Bag of Words, CBOW) and predict the preceding and following words, given a word (Skip-Gram). Image from \brackettext{\cite{DBLP:journals/corr/abs-1301-3781}}.}
\label{fig:skip_gram}
\end{figure}
If sufficient data is used for training, word2vec can predict with high accuracy the word's meaning based on its surrounding words in the corpus and also successfully capture semantic relations, such as country and capital relations, as well as syntactic relations. For example, present tense and past tense, or even meaningful algebraic operations are supported. For example vector of  \emph{``man''} will have approximately the same distance to  \emph{``brother''} as \emph{``women''} to \emph{``sister''}: $V_{brother}-  V_{man} +V_{women} \approx V_{sister}$. There exists numerous variation of this model, from taking the character level information into account ~\brackettext{\cite{DBLP:journals/corr/BojanowskiGJM16}} to incorporating the statistics of the whole corpus (GloVe model)~\brackettext{\cite{DBLP:conf/emnlp/PenningtonSM14}}. GloVe model in particular speeds up the learning and when trained on a large corpus tends to generate more promising results on downstream tasks such as named entity recognition.\\
Word2vec based methods, although good at capturing semantics, have some drawbacks. They treat each word equally regardless of their type (e.g., person or location). Although many information extraction and natural language processing tasks rely on named entities, this information is missing from the normal word embeddings. Considering the type of the word, while generating the word embeddings, provides additional information and helps to create more intelligent features to be used by a downstream task that operate on named entities. A term-based model fails to distinguish between entity homographs such as  \emph{``Paris''}  which can refer to the city or \emph{``Paris Hilton''} and cannot recognize any compound word. Any word separated by space is considered its own token, whereas many companies and actors name have two parts that not necessarily separate. To create an embedding for an actor like \emph{``Paris Hilton''} separate embeddings for \emph{``Paris''} and \emph{``Hilton''} have to be combined in some way to generate a new representation. Although there exists some work on learning embedding for phrases~\brackettext{\cite{DBLP:journals/tacl/HillCKB16}}~\brackettext{\cite{DBLP:conf/acl/YinS14}} or combining them after training with averaging, adding, or multiplying, none the methods focus on entities and the combination techniques are far from satisfactory~\brackettext{\cite{DBLP:journals/cogsci/MitchellL10}}~\brackettext{\cite{DBLP:conf/eacl/DurmeRPM17}}.  \\
One valuable tool in NLP that takes this information into account is implicit networks of entities extracted from an unstructured text as demonstrated in the work of~\brackettext{\cite{Rousseau:2013:GTN:2505515.2505671}}. These networks, similar to knowledge graphs, encode relationships between entities but unlike them, information of the type of connection is unknown. On the other hand, the strength of relationships can be inferred from edge weights. In an implicit entity network, statistics of a corpus of text is encoded in the edge weights between entities of different types, e.g, actor, location, organization, date, and term. With a network, similar to word2vec, many relations between nearby words can be extracted based on edges and nearby nodes. Moreover, the type-specific nature of the network allows for more flexibility and encodes more information than a sliding window with a fixed size. 

 \section{How to Interpret Word Embeddings? }

Dense vector representations of words can encode underlying complexity and latent trends in the data, but they are far from being interpretable. In many cases, the semantic structure is heterogeneously distributed across the embedding dimensions, which makes interpretation challenging. 
Recently, some works have been done to make the embedding less ambiguous. In the work of \brackettext{\cite{DBLP:journals/corr/FaruquiTYDS15}} transformation of word vectors into sparse (and optionally binary) vectors was proposed. Each vector is projected into an over-complete binary vector, where each dimension represents a feature similar to ones used in traditional NLP systems but found automatically during training. $0$ or $1$ indicates the presence or absence of a certain feature. However, since their dimensions are binary valued, there is no notion of the extent to which a word participates in a particular dimension. Rotating the word vectors was introduced by \brackettext{\cite{prak2017}} to improve the interpretability.  \brackettext{\cite{DBLP:journals/corr/abs-1711-08792}} proposes a neural network-based approach to the problem by using sparse Auto-encoders. 
Nevertheless, both of these methods are applied as a later processing step to the trained embedding and offer no solution to learn the embeddings as an interpretable component during training time. Moreover, the meaning of a specific dimension has been investigated based on the words that most activate it. It is still unknown, if the model predicts \emph{``London"} and \emph{``Berlin"} as similar, whether they both appeared in same the context location-wise, e.g., whether both are capitals and are situated in Europe or because they share the same organizations, e.g., many companies moved their offices from London to Berlin after the Brexit. This information can to some extent be extracted from implicit networks, where the entity type is specified for each node. If these networks encode all the important words in the vocabulary, they can form a graph-based representation of the corpus, which can be used instead of a textual corpus to generate embeddings.\\
In the following, we propose a method to combine the benefits of text annotated and implicit networks with word embedding techniques to generate vector representation for terms and entities. 

\section{Objective and Contributions}
In this study, try to create type sensitive word embeddings by training entity and term embeddings jointly,  on an annotated corpus or co-occurrence graph. In addition, to further analyze the importance of each type we experiment with facetted embeddings as a method to better understand the semantic structures and increase the interpretability of vector space models. \\
To generate entity embeddings, a vector representation of named entities and terms in an annotated document, we use either a corpus of text with annotation or an implicit network extracted from it. Unlike traditional methods, where all tokens are treated as terms, entity embeddings take the type information into account. One advantage of a disambiguated corpus is the distinction between entity homographs. As a result, a vector representation of \emph{``Washington"} as a person is different from the same word as location. Unlike raw text, where the compound name is broken down into specific parts, names of entities will be grouped into a unique identifier that helps the model to learn a distinct representation for it. As a result, we will have a unique vector for words such as  \emph{``Barack Obama"}, instead of two separate \emph{``Barack"} and \emph{``Obama"} representations that may not refer to the same person. Aliases are variant names of the same entity. In a disambiguated document all the aliases are treated as unit identity, e.g. \emph{``Obama"} and \emph{``Mr. President"} in the time frame after $2009$. A term-based model will combine treat the term \emph{``Mr. President"} the same throughout all the time-span, creating an ambiguous representation of it. Resolving aliases also creates a more word context pair for each entity, which can increase performance. For example, all the words surrounding \emph{``Obama"} or any of its aliases will be considered as context word for the entity \emph{``Barack Obama"}. \\
\noindent
Facetted embedding is a vector space model, in which each part of the vector represents different properties of the word, namely locations, actors or organizations associated with it, dates of its mention and terms that define its meaning. One visual example of such an embedding ($V$) is shown below: \\
\mathleft
\begin{equation}
V=\left[ \underbrace { \begin{matrix}{ a }_{ 1,1 } ... { a }_{ 1,M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 1,M+1 } ... { a }_{ 1,2M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 1,2M+1 } ... { a }_{ 1,3M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 1,3M+1 } ... { a }_{ 1,4M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 14M+1 } ... { a }_{ 1,5M } \end{matrix} }  \right] 
\label{eq:concat_vec}
\end{equation}
$$ \quad  ACT \quad  \qquad  LOC\qquad \qquad ORG\qquad \quad \qquad DAT\qquad \quad  \qquad  TER\qquad \qquad$$
\mathcenter
\\
The type-information of an entity is encoded in a named entity graph. The facetted embedding takes as input the edge weights of the implicit network along with the type information and generates a separate embedding structure for each word based on the surrounding entities. In contrast to simple word2vec, each part of the embedding vector is responsible for encoding one of the available entity types, e.g., the actors part only encodes the actors in the context of a word.\\
The advantage of this approach is that the similarity of the words can now be broken down into their different attributes. If two entities are closer in location space and further in actors space, it is possible that they are situated close locally, but are not related to the same people. These embedding will be a useful tool to explore and understand changes in news streams. Exploring and visualizing different dimensions of an embedding over time will give us insight into how that entity has evolved. For example, it is expected that since \emph{``Donald Trump"} has become president, the location dimension of his embedding changes dramatically as he has moved to White House and travels more often. The organizations, however, tend to stay relatively stable as his business did not undergo dramatic changes. Without the separation of dimensions, it is hard to identify in which particular aspect has the entity most transformed. In addition, since dimensions are independent and can be analyzed separately, the similarity between two entities becomes more interpretable. It would be possible to identify if two similar entities are mentioned in the same locations or in the context of related actors. \\
Separate dimensions also provide flexibility in information retrieval tasks. Facetted embeddings encode the type of the entities surrounding a word. This type-specific information can be used in the search, where one can query for entities that are closer to a certain word in the temporal or location aspect. The normal embedding treats all the word equally and therefore, can not reflect this type of similarity. Since the dimensions are independent and can be combined in an arbitrary way, a combination of different dimensions can tailor the search results further and create a type-specific search. \\
Recent work in entity disambiguation uses pre-trained word embedding in combination with the recurrent neural network to enhance the accuracy of the system~\brackettext{\cite{DBLP:journals/corr/LampleBSKD16}}. In the case of news data, entities play an important role in event detection and extraction of temporal facts. If the type of an entity is known, search in the embedding space becomes more flexible, allowing for type-specific search or removal of terms. For example, when looking for the nearest neighbor to a word, the neighborhood can be filtered to contain only entities of a specific type or remove the entities altogether and look at only at surrounding terms. One can also search the relation between people or organizations, which is missing from the term-based models. 
In addition, in the case of facetted models embedding space can be broken down to its individual components and studied separately. For example, to \emph{``Washington"} as a political person tends to appear more often with the same actors and has different actor dimension than \emph{``Washington DC"} as a location.  \\
Training word embedding for a specific task such as the analysis of news articles is by no means a novel task. Word embeddings have been tailored to for dependency parsing (analysis of the grammatical structure of a sentence and establishing relationships between words)~\brackettext{\cite{P14-2131}} and semantic relation classification \brackettext{\cite{DBLP:journals/corr/HashimotoSMT15}.} But none of the methods create separate interpretable dimensions and their usage is mostly limited to the tasked they are trained for. In this thesis, we aim to introduce entity embeddings and the facetted model as a general framework for learning embeddings with known types and separable dimensions, where their usage is not limited to only to the analysis of news articles but can be trained on any corpus and can be tailored further to match a specific downstream task. We aim to analyze how the annotation can affect the quality of embeddings in popular evaluation tasks and compare their weaknesses and strength against the normal word embeddings techniques. 

\section{Overview}
The remainder of this thesis is structured as follows: In Chapter~\ref{chap:background}, the background information required for the model and related work is discussed, with a brief introduction to basics of NLP and neural networks. Well-established word embedding and graph embedding methods, which are the base of the entity and facetted embeddings are explained in detail in this chapter as a pre-requisite for understanding how the final models work. In Chapter~\ref{chap:entity}, two types of models for learning terms and entity embeddings jointly on an annotated corpus is presented. One method focuses on learning the vector representation from a textual corpus, while the other method requires an extracted implicit network. Chapter~\ref{chap:main} contains an in-depth definition of the facetted embeddings, same as the entity embeddings, different models are proposed based on the input data (textual or graph-based). The \ref{chap:eval}th chapter contains the test cases and evaluation results. The entity embeddings and facetted models are analyzed using visualization s and also tested against well-known analogy, word similarity and categorization datasets. The work is closed with a conclusion and outlook on possible future work in Chapter~\ref{chap:concl}. 
