%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{chap:intro}
The long-standing challenge for research in computer science is the understanding of written text and extraction of useful information from it. Text that is created by humans is often unstructured and ambiguous, whereas machine learning algorithms typically prefer fixed-length inputs and outputs. Therefore, machine learning algorithms cannot work with raw text directly and the text has to be converted into vectors of features.
\\
\begin{figure}
\centering 
\resizebox{0.3\textwidth}{0.25\textwidth}{      
\input{images/vector_space_models.tex}
}
\caption{An example of dense vector presentation of words in a 2-dimensional space, that maps words with similar meaning to nearby points.}
\label{fig:vsmodels}
\end{figure}
Traditional \emph{Natural Language Processing} (NLP) systems treat words as atomic units. Represented as vectors, each word would be treated as a one-hot vector (with $1$ in a single position and zero in rest) the size of the vocabulary. These vectors are sparse and orthogonal, and therefore contain no notion of similarity among themselves. For example, if a user is searching for \emph{``Heidelberg Hotel''}, documents containing \emph{``Heidelberg Motel''} are to be disregarded since the dot product of the one-hot vectors of \emph{``Hotel''} and \emph{``Motel''} is zero. On the other hand, distributed vector space models represent words in a continuous vector space, where a word is represented by its relation to the surrounding words. These models are based on the idea that similar words tend to occur in a similar context or as the British linguist, J.R. Firth, said:\\ \\
\noindent
\say{You shall know a word by the company it keeps.} (J.R. Firth 1957)\\
\\
These distributed representations try to map words to a dense vector, such that words with closer meanings are mapped to the nearby points and the similarity between them can be computed based on their distance in space. An example of the embedding space for such models is illustrated in Figure~\ref{fig:vsmodels}, where the vectors of \emph{``dog"},  \emph{``pet"} and, \emph{``labrador"} are mapped to nearby points and are farther away from unrelated words like \emph{``snowboard"}.
Learning dense vector representations of words dates back to $2000$ in the paper by~\brackettext{\cite{DBLP:conf/nips/BengioDV00}}. The first simple and scalable algorithm, however, is \emph{word2vec} presented by \brackettext{\cite{DBLP:journals/corr/abs-1301-3781}}, where the aim is to capture the meaning of a word based on the surrounding context. There exists numerous variations of this model, from taking the character level information into account ~\brackettext{\cite{DBLP:journals/tacl/BojanowskiGJM17}} to incorporating the statistics of the whole corpus, namely the GloVe model~\brackettext{\cite{DBLP:conf/emnlp/PenningtonSM14}}. The GloVe model in particular speeds up the learning and when trained on a large corpus tends to generate more promising results on downstream tasks such as named entity recognition.\\
Methods based on word2vec, despite being good at capturing semantics, have some drawbacks. They treat all words equally as terms and cannot be directly used to represent named entities. \emph{Named Entities (NEs)} refer to proper names or nouns and numerical
expressions, such as dates and quantities~\brackettext{\cite{DBLP:conf/lrec/ToralMM08}}.
Many NLP tasks either depend on or use named entities as input. Identifying these references and classifying them has given rise to named entity recognition and disambiguation tasks. They are also extensively used in \emph{Information Retrial (IR)} and search engines. Despite their importance, an embedding model for named entities does not exist yet. Disregarding the named entities while generating an embedding creates several challenges for downstream tasks that use them as input. Named entities often contain compound words and phrases, which if not grouped together from the start result in ambiguous and unclear embeddings. Although some work has been done on embedding phrases~\brackettext{\cite{DBLP:journals/tacl/HillCKB16,DBLP:conf/acl/YinS14}} or combining compound words after training with averaging, adding, or multiplying~\brackettext{\cite{DBLP:conf/eacl/DurmeRPM17,DBLP:journals/cogsci/MitchellL10}}, none of these focus on entities. \\
A term-based model fails to distinguish between entity homographs such as  \emph{``Paris''},  which can refer to the city, or \emph{``Paris Hilton''} and cannot recognize any compound word. Any word separated by space is considered its own token, whereas many companies and actors name have two parts that not necessarily separate.  In a team-based model, if an entity is repeated several times with different names, a term-based model treat each name independently. In doing so, we lose valuable context information for the entity. Another drawback of term-based models is their inability to deal with date information. There exist multiple representations of date and time in text, e.g, $1$ January 1998 or $1998-01-01T15:53:00$, most models disregard any numeric information in text and limit themselves to a certain alphabet. With the exception of a few studies that try to add a temporal dimension to a paragraph or word embeddings, they mostly focus on how the embeddings evolve or change with time and do not achieve a vector presentation of dates~\brackettext{\cite{DBLP:conf/coling/MirzaT16a,DBLP:conf/ecir/YoonMWLK18}}.  \\
Another drawback of word embeddings in general is lack of interpretability. In many cases, the semantic structure is heterogeneously distributed across the embedding dimensions, which makes interpretation challenging. Recently, some research has been done to make the embedding less ambiguous~\brackettext{\cite{DBLP:journals/corr/FaruquiTYDS15,DBLP:conf/emnlp/ParkBO17,DBLP:conf/aaai/SubramanianPJBH18}}. However, none of them show any insight about possible entity relations and most are applied as a post-processing step to the trained embedding. They offer no solution to learn the embeddings as an interpretable component during training time. There is no efficient method that can produce interpretable embeddings based on the entities surrounding a word. It is still unknown, if the model predicts \emph{``London''} and \emph{``Berlin''} as similar, whether they both appeared in the same context location-wise, e.g., whether both are capitals and are situated in Europe or because they share the same organizations, e.g., many companies planned to move their offices from London to Berlin after the Brexit. \\
On the other hand, annotating the text with named entities, requires extensive pre-processing, from sentence splitting and tokenization to part-of-speech tagging. Not only these steps are prone to errors, but also the task of recognizing and classifying the entities, even with the state-of-the-art tools are far from perfect. Therefore, naively applying the word embedding methods to entity annotated text might have more problems than benefits. It is important that the drawbacks of such an approach are analysed. 

% BR
%
% Good introduction, but it gets quickly bogged down in details.
% Everything important is there but you should rather present a single
% use case and expand from there. The idea of interspersing related work
% within the introduction is nice, but this could also become a separate
% section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Objective and Contributions}
In this thesis, we address the problems of team-based models for generating embeddings for named entities as well as terms using an annotated corpus. In addition, to further analyze the importance of each entity type we experiment with faceted embeddings as a method to better understand the semantic structures and increase the interpretability of vector space models. Therefore, this thesis discusses two main models, one for the entity and term embeddings and one for the faceted model. Below, we will discuss the contribution of each separately.\\
In analysing the entity-based models make the following contributions:
\begin{itemize}
\item We apply the state-of-the-art word embedding methods on annotated corpus to train terms and embeddings, jointly. 
\item We use graph embedding techniques to embed the nodes of a co-occurrence graph, extracted from the annotated text as an alternative means to achieve entity embeddings. 
\item We evaluate both of our models against the well-established word embedding methods on a set of intrinsic evaluation tasks. 
\item Since few test datasets contain any entities, we create some of our own, to better analyse the effect of entity annotation. 
\item Through visualization and experimental analysis, we investigate the underlying semantics that entity embeddings capture and discuss the implications for entity-centric downstream tasks.
\item By comparing the entity-based and word-based methods, we discuss the strength, weaknesses and application scenarios for both models.
\end{itemize}
To further analyse the impact of different types of entities on a certain entity embedding and to make the embeddings more interpretable we introduce faceted embedding. Our faceted model makes the following contributions: 
\begin{itemize}
\item We modify the well-established word embedding techniques to separable parts, where each part of the embedding corresponds to the word relating to a specific type of entities. 
\item We use graph embedding techniques on a co-occurrence graph extracted from an annotated corpus to examine the relation of each word to the neighbours of a specific type. With embeddings the nodes of such a graph we introduce an alternative way to achieve the faceted model. 
\item We evaluate the faceted models against our entity-based models and also word-based methods on raw text, based on word-centric intrinsic tasks. 
\item Since separate parts of embeddings divide the embedding space into the different type of entities available in text, we investigate the effect of each type on intrinsic tasks. 
\item With visualizing the embedding parts separately, we explore the potential use case and underlying semantics of such models. 
\item We discuss the drawbacks and advantages of faceted models. 
  
\end{itemize}

% BR: A good section, but the structure should be more succinct. When
% this is read (by the grader), I should immediately see what the
% objectives were and how they have been achieved. Maybe a different
% layout with 'at-a-glance' contributions would be helpful here. It is
% good that you use concrete examples, but there should be a brief
% summary about them.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview of the Thesis Structure}
The remainder of this thesis is structured as follows: 
In Chapter~\ref{chap:background},  we discuss the background information required for the model and related work, with a brief introduction to the basics of NLP and neural networks. This discussion is followed by, explanation of well-established word embedding and graph embedding methods, which are the base of the entity and faceted embeddings.\\
 In Chapter~\ref{chap:entity}, two types of models for learning terms and entity embeddings jointly on an annotated corpus are presented. One method focuses on learning the vector representation from a textual corpus, while the other method requires a co-occurrence graph extracted from the annotated text.\\
Chapter~\ref{chap:faceted} contains an in-depth definition of the faceted embeddings, same as the entity embeddings, different models are proposed based on the input data (textual or graph-based). \\
In Chapter~\ref{chap:eval}, the test cases and evaluation results are presented. The entity embeddings and faceted models are analysed using visualizations and also tested against well-known analogy, word similarity and categorization datasets. \\
The work is closed with a conclusion and outlook on possible future work in Chapter~\ref{chap:concl}. 


% BR: Very nice in total. I would recommend the following, though:
%  - make the claims more concise and precise
%  - formulate the objectives and how this was achieved.
%  - ideally, you can formulate both at the same time and place
%  - for example, I had 'Contributions' in my dissertation as well, you
%    may want to look at these (text me if you have troubles finding
%    that)
