%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{chap:intro}
The long-standing challenge for research in computer science is the understanding of written text and extraction of useful information from it. Text that is created by humans is often unstructured and ambiguous, whereas machine learning algorithms typically prefer fixed-length inputs and outputs. Therefore, machine learning algorithms cannot work with raw text directly and the text has to be converted into vectors of features.
\\
Traditional \emph{Natural Language Processing} (NLP) systems treat words as atomic units. Represented as vectors, each word would be treated as a one-hot vector (with $1$ in a single position and zero in rest) the size of the vocabulary. These vectors are sparse and orthogonal, and therefore contain no notion of similarity among themselves. On the other hand, distributed vector space models represent words in a continuous vector space, where a word is represented by its relation to the surrounding words. These models are based on the idea that similar words tend to occur in a similar context or as the British linguist, J.R. Firth, said:\\ \\
\noindent
\say{You shall know a word by the company it keeps.} (J.R. Firth 1957)\\
\\
These distributed representations try to map words to a dense vector, such that words with closer meanings are mapped to the nearby points and the similarity between them can be computed based on their distance in space.
Learning dense vector representations of words dates back to $2000$ in the paper by~\brackettext{\cite{DBLP:conf/nips/BengioDV00}}. The first simple and scalable algorithm, however, is \emph{word2vec} presented by \brackettext{\cite{DBLP:journals/corr/abs-1301-3781}}, where the aim is to capture the meaning of a word based on the surrounding context. There exists numerous variations of this model, from taking the character level information into account ~\brackettext{\cite{DBLP:journals/tacl/BojanowskiGJM17}} to incorporating the statistics of the whole corpus, namely GloVe model~\brackettext{\cite{DBLP:conf/emnlp/PenningtonSM14}}. The GloVe model in particular speeds up the learning and when trained on a large corpus tends to generate more promising results on downstream tasks such as named entity recognition.\\

Methods based on word2vec, despite being good at capturing semantics, have some drawbacks. They treat all words equally as terms and cannot be directly used to represent named entities. \emph{Named Entities (NEs)} refer to proper names or nouns and numerical
expressions, such as dates and quantities~\brackettext{\cite{DBLP:conf/lrec/ToralMM08}}.
Many NLP tasks either depend on or use named entities as input. Identifying these references and classifying them has given rise to named entity recognition and disambiguation tasks. They are also extensively used in \emph{Information Retrial (IR)} and search engines. Despite their importance, an embedding model for named entities does not exist yet. Disregarding the named entities while generating an embedding creates several challenges for downstream tasks that use them as input. Named entities often contain compound words and phrases, which if not grouped together from start result to ambiguous and unclear embeddings. Although some work has been done on embedding phrases~\brackettext{\cite{DBLP:journals/tacl/HillCKB16,DBLP:conf/acl/YinS14}} or combining compound words after training with averaging, adding, or multiplying~\brackettext{\cite{DBLP:conf/eacl/DurmeRPM17,DBLP:journals/cogsci/MitchellL10}}, but none of these focus on entities. \\
A term-based model fails to distinguish between entity homographs such as  \emph{``Paris''}  which can refer to the city or \emph{``Paris Hilton''} and cannot recognize any compound word. Any word separated by space is considered its own token, whereas many companies and actors name have two parts that not necessarily separate.  In a team-based model, if an entity is repeated several times with different names, a term-based model treat each name independently. In doing so, we lose valuable context information for the entity. Another drawback of term-based models is their inability to deal with date information. There exist multiple representations of date and time in text, e.g, $1$ January 1998 or $01-01-1998$, most models disregard any numeric information in text and limit themselves to a certain alphabet. With the exception of a few studies that try to add a temporal dimension to a paragraph or word embeddings, they mostly focus on how the embeddings evolve or change with time and do not achieve a vector presentation of dates~\brackettext{\cite{DBLP:conf/coling/MirzaT16a,DBLP:conf/ecir/YoonMWLK18}}.  \\
Another drawback of word embeddings in general is lack of interpretability. In many cases, the semantic structure is heterogeneously distributed across the embedding dimensions, which makes interpretation challenging. Recently, some research has been done to make the embedding less ambiguous~\brackettext{\cite{DBLP:journals/corr/FaruquiTYDS15,DBLP:conf/emnlp/ParkBO17,DBLP:conf/aaai/SubramanianPJBH18}}. However, none of them show any insight about the possible entity relations and most are applied as a post-processing step to the trained embedding. They offer no solution to learn the embeddings as an interpretable component during training time. There is no efficient method that can produce interpretable embeddings based on the entities surrounding a word. It is still unknown, if the model predicts \emph{``London''} and \emph{``Berlin''} as similar, whether they both appeared in the same context location-wise, e.g., whether both are capitals and are situated in Europe or because they share the same organizations, e.g., many companies moved their offices from London to Berlin after the Brexit. \\
On the other hand, annotating the text with named entities, requires extensive pre-processing, from sentence splitting and tokenization to part-of-speech tagging. Not only these steps are prone to errors, but also the task of recognizing and classifying the entities, even with the state-of-the-art tools are far from perfect. Therefore, naively applying the word embedding methods to entity annotated text might have more problems than benefits. It is important that the drawbacks of such an approach is analysed. 

% BR
%
% Good introduction, but it gets quickly bogged down in details.
% Everything important is there but you should rather present a single
% use case and expand from there. The idea of interspersing related work
% within the introduction is nice, but this could also become a separate
% section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Objective and Contributions}
In this thesis, we address the problems of team-based models for generating embeddings for named entities as well as terms using an annotated corpus. In addition, to further analyze the importance of each entity type we experiment with faceted embeddings as a method to better understand the semantic structures and increase the interpretability of vector space models. Therefore, this thesis discusses two main models, one for the entity and term embeddings and one for the faceted model. Below, we will discuss the contribution of each separately.\\
In analysing the entity-based models make the following contributions:
\begin{itemize}
\item We apply the state-of-the-art word embedding methods on annotated corpus to train terms and embeddings, jointly. 
\item We use graph embedding techniques to embed the nodes of a co-occurrence graph, extracted from the annotated text as an alternative means to achieve entity embeddings. 
\item We evaluate both of our models against the well-established word embedding methods on a set of intrinsic evaluation tasks. 
\item Since few test datasets contain any entities, we create some of our own, to better analyse the effect of entity annotation. 
\item Through visualization and experimental analysis, we investigate the underlying semantics that entity embeddings capture and discuss the implications for entity-centric downstream tasks.
\item By comparing the entity-based and word-based methods, we discuss the strength, weaknesses and application scenarios for both models.
\end{itemize}
To further analyse the impact of different types of entities on a certain entity embedding and to make the embeddings more interpretable we introduce faceted embedding. Our faceted model makes the following contributions: 
\begin{itemize}
\item We modify the well-established word embedding techniques to separable parts, where each part of the embedding corresponds to the word relating to a specific type of entities. 
\item We use graph embedding techniques on a co-occurrence graph extracted from an annotated corpus to examine the relation of each word to the neighbours of a specific type. With embedding the nodes of such graph we introduce an alternative way to achieve the faceted model. 
\item We evaluate the faceted models against our entity-based models and also word-based methods on raw text, in word-centric intrinsic tasks. 
\item Since separate parts of embeddings divide the embedding space into the different type of entities available in text, we investigate the effect of each type on intrinsic tasks. 
\item With visualizing the embedding parts separately, we explore the potential use case and underlying semantics of such models. 
\item We discuss the drawbacks and advantages of faceted models. 
  
\end{itemize}

% BR: A good section, but the structure should be more succinct. When
% this is read (by the grader), I should immediately see what the
% objectives were and how they have been achieved. Maybe a different
% layout with 'at-a-glance' contributions would be helpful here. It is
% good that you use concrete examples, but there should be a brief
% summary about them.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}
The remainder of this thesis is structured as follows: 
In Chapter~\ref{chap:background},  we shall discuss the background information required for the model and related work, with a brief introduction to the basics of NLP and neural networks. This discussion is followed by, explanation of well-established word embedding and graph embedding methods, which are the base of the entity and faceted embeddings.\\
 In Chapter~\ref{chap:entity}, two types of models for learning terms and entity embeddings jointly on an annotated corpus are presented. One method focuses on learning the vector representation from a textual corpus, while the other method requires a co-occurrence graph extracted from the annotated text.\\
Chapter~\ref{chap:faceted} contains an in-depth definition of the faceted embeddings, same as the entity embeddings, different models are proposed based on the input data (textual or graph-based). \\
The \ref{chap:eval}th chapter contains the test cases and evaluation results. The entity embeddings and faceted models are analysed using visualizations and also tested against well-known analogy, word similarity and categorization datasets. \\
The work is closed with a conclusion and outlook on possible future work in Chapter~\ref{chap:concl}. 


% BR: Very nice in total. I would recommend the following, though:
%  - make the claims more concise and precise
%  - formulate the objectives and how this was achieved.
%  - ideally, you can formulate both at the same time and place
%  - for example, I had 'Contributions' in my dissertation as well, you
%    may want to look at these (text me if you have troubles finding
%    that)



%parts removed : 
%-------------------------------------------------
%\item They can distinguish between entity homographs. As a result, a vector representation of \emph{``Washington''} as a person is different from the same word as location.\\ 
%\item Unlike raw text, where the compound name is broken down into specific parts, names of entities are \deleted[id=br]{be} grouped into a unique identifier that helps the model to learn a distinct representation for it. As a result, we have a unique vector for words such as  \emph{``Barack Obama''}, instead of two separate \emph{``Barack''} and \emph{``Obama''} representations that may not refer to the same person. \\
%\item Mentions of the same entity with different names are considered a unit identity, e.g., \emph{``Obama''} and \emph{``Mr. President''} in the time frame after $2009$, which creates a \deleted[id=br]{more} word context pair for each entity, which can increase performance. For example, all the words surrounding \emph{``Obama''} or any of \deleted[id=br]{their} aliases are considered \replaced[id=br]{to be a}{as} context word for the entity \emph{``Barack Obama''}. 
%\item We identify expressions which are dates or time, and normalise them into a single unique date. Hence, we can incorporate\deleted[id=br]{d} the temporal information as the context of a word, \added[id=br]{and} at the same time learn a vector representation for the dates. \\
%\item Since entities have pre-defined types, such as actor or location, we can perform a more effective search in the embedding space. The neighborhood of a word can be filtered to contain only relevant actors or locations, which is beneficial for finding relationship\added[id=br]{s} between people or organizations. \\
%\item In a team-based model, a word is only defined by the terms surrounding it. With annotated named entities now we can have different definitions of a word, based on the type of entities surrounding it. A word can be defined by the most relevant locations or actors. For example, when looking for the nearest neighbor of a word, the neighborhood can be filtered to contain only entities of a specific type or remove the entities altogether and look only at surrounding terms. We take advantage of this property and introduce a new search criterion for analogy tasks that succeeds in capturing entity-entity relations better than team-based models.    
%-------------------------------------------------
%One valuable tool in NLP that takes this information into account is implicit networks of entities extracted from an unstructured text as demonstrated in the work of~\brackettext{\cite{DBLP:conf/cikm/RousseauV13}}. These networks, similar to knowledge graphs, encode relationships between entities but unlike them, information of the type of connection is unknown. On the other hand, the strength of relationships can be inferred from edge weights. In an implicit entity network, statistics of a corpus of text is encoded in the edge weights between entities of different types, e.g, actor, location, organization, date, and term. With a network, similar to word2vec, many relations between nearby words can be extracted based on edges and nearby nodes. Moreover, the type-specific nature of the network allows for more flexibility and encodes more information than a sliding window with a fixed size. 

%DBLP:conf/nips/BengioDV00 Learning dense vector representations of words dates back to $2000$ in which a feature vector, much smaller than the size of the vocabulary, was used to express words using a probabilistic model. 
%The work of~\brackettext{\cite{DBLP:journals/jmlr/CollobertWBKKK11}} proved that word representation could not only be achieved through probabilistic models, but that neural network %architectures can also learn these internal representations from vast amounts of mostly unlabeled training data.\\

%-------------------------------------------------

%The goal of the model is, given a center word, to predict the words that occur in its surroundings.
% If sufficient data is used for training, word2vec can predict with high accuracy the word's meaning based on its surrounding words in the corpus and also successfully capture semantic relations, such as country and capital relations, as well as syntactic relations. For example\added[id=br]{, the} vector representation of  \emph{``man''} \replaced[id=br]{has}{have} approximately the same distance to  \emph{``brother''} as \emph{``women''} to \emph{``sister''}. There exists numerous variations of this model, from taking the character level information into account ~\brackettext{\cite{DBLP:journals/tacl/BojanowskiGJM17}} to incorporating the statistics of the whole corpus (GloVe model)~\brackettext{\cite{DBLP:conf/emnlp/PenningtonSM14}}. The GloVe model in particular speeds up the learning and when trained on a large corpus tends to generate more promising results on downstream tasks such as named entity recognition.\\%
%For example, if a user is searching for \emph{``Heidelberg Hotel''}, documents containing \emph{``Heidelberg Motel''} are \added[id=br]{to} be disregard\added[id=br]{ed} since the dot product of the one-hot vectors of \emph{``Hotel''} and \emph{``Motel''} is zero.%
%-------------------------------------------------

% In other words, any information unit like names, including person, organization and location names, and numeric expressions including time, date and money, which can be identified and categorized are called named entities~\brackettext{\cite{SCHOL:journal/LI/nadeau}}. More formally, if we consider all the words as terms and define $T$ as the set of all terms in the vocabulary, $N\subseteq T$ is the set of all entities that fall into a pre-defined category.%

%-------------------------------------------------

%In the work of~\brackettext{\cite{DBLP:journals/corr/FaruquiTYDS15}} transformation of word vectors into sparse (and optionally binary) vectors was proposed. Each vector is projected into an over-complete binary vector, where each dimension represents a feature similar to ones used in traditional NLP systems but found automatically during training. However, since their dimensions are binary valued, there is no notion of the extent to which a word participates in a particular dimension. Rotating the word vectors was introduced by~\brackettext{\cite{DBLP:conf/emnlp/ParkBO17}} in order to improve the interpretability.  \brackettext{\cite{DBLP:conf/aaai/SubramanianPJBH18}} proposes a neural network-based approach to the problem by using sparse auto-encoders.%

%--------------------------------------------------

%In the remainder of this thesis, we aim to introduce entity embeddings and the faceted model as a general framework for learning embeddings with known types and separable dimensions. For entity embedding we propose two approaches. (a) by training the well-established word embedding models on annotated data. (b) by taking advantage of co-occurrence graphs extracted from the annotated text. We modify the well-established models for word embeddings to generate the separation of the types and achieve the faceted model. The faceted models are also analysed in two categories of (a) modified word embedding models on annotated data. (b) use of co-occurrence graphs extracted from the annotated text. We analyze how the annotation can affect the quality of embeddings in popular evaluation tasks and compare their weaknesses and strength against the normal word embeddings techniques. 

%--------------------------------------------------

%To generate a vector representation of named entities and terms in an annotated document, we use either a corpus of text with entity annotations or an co-occurrence graph extracted from it. Unlike traditional methods, where all tokens are treated as terms, entity embeddings take the type information into account.

%-------------------------------------------------
 %faceted embedding is a vector space model, in which each part of the vector represents different properties of the word, namely locations, actors or organizations associated with it, dates of its mention and terms that define its meaning. One visual example of such an embedding ($w$) for a corpus containing types of actor (ACT), location (LOC), organisation (ORG), date (DAT) and term (TER) is shown below : \\
%\mathleft
% BR: You should use \mathrm{} for ACT, LOC, and so on, in the formula.
% Otherwise, it is typeset in the wrong font.
%\begin{equation}
%w=\left[ \underbrace { \begin{matrix}{ a }_{ 1,1 } ... { a }_{ 1,M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 1,M+1 } ... { a }_{ 1,2M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 1,2M+1 } ... { a }_{ 1,3M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 1,3M+1 } ... { a }_{ 1,4M } \end{matrix} } |\underbrace { \begin{matrix}{ a }_{ 14M+1 } ... { a }_{ 1,5M } \end{matrix} }  \right] 
%\label{eq:concat_vec}
%\end{equation}
%$$ \quad  ACT \quad  \qquad  LOC\qquad \qquad ORG\qquad \quad \qquad DAT\qquad \quad  \qquad  TER\qquad \qquad$$
%\mathcenter

%-------------------------------------------------

%(a) In contrast to traditional word embedding models, each part of the embedding vector is responsible for encoding one of the available entity types, e.g., the actors part only encodes the actors in the context of a word, which results in a more interpretable representation of words. (b) With this separation of the types, the similarity of the words can be broken down into their different attributes. If two entities are closer in location space and farther in actors space, it is possible that they are situated close locally, but are not related to the same people. (c) These embedding\added[id=br]{s} are a useful tool to explore and understand changes in news streams. Exploring and visualizing different dimensions of an embedding over time  gives us insight\added{s} into how that entity has evolved. For example, it is expected that since \emph{``Donald Trump''} has become president \added{of the USA}, the location dimension of his embedding changes dramatically as he has moved to White House and travels more often. The organizations, however, tend to stay relatively stable as his business did not undergo dramatic changes. Without the separation of dimensions, it is hard to identify what kind of transformation the particular entity has undergone. (d) Since dimensions are independent and can be analyzed separately, the similarity between two entities becomes more interpretable. It would be possible to identify if two similar entities are mentioned in the same locations or in the context of related actors. (e) Separate dimensions also provide flexibility in information retrieval tasks. faceted embeddings encode the type of the entities surrounding a word. This type-specific information can be used in the search, where one can query for entities that are closer to a certain word in the temporal or location aspect. For example, \emph{``Washington''} as a political person tends to appear more often with the same actors and has \added{a} different actor dimension than \emph{``Washington DC''} as a location. The standard embedding treats all the word equally and therefore\deleted{,} can\deleted{ }not reflect this type of similarity. Since the dimensions are independent and can be combined in an arbitrary way, a combination of different dimensions can tailor the search results further and create a type-specific search. 