%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}\label{chap:concl}

In this thesis, we investigated different techniques to jointly train embeddings for terms and entities on annotated corpus with named entities. We considered the naive application of the popular models word2vec and Glove to annotated texts, as well as embedding the nodes of co-occurrence graphs extracted from the annotated text. We used popular node embedding methods DeepWalk and VERSE and tweaked them to achieve the entity embeddings. Furthermore, to analyse the effect of each entity type on the embedding space, we explored variations of GloVe, word2vec, and DeepWalk that separate the embedding space into different embedding types. These faceted models consist of separable components, where each component indicates the relation of the word to entities of a specific type. Moreover, we compared all the proposed models to traditional word embeddings on a comprehensive set of term-focused evaluation tasks and performed entity-centric exploration to identify the strengths and weaknesses of each approach. \\
We found that even though training the state-of-the-art word embeddings directly on annotated text is possible, their performance degrades in term-centric tasks, in particular in tasks that depend on relatedness. On the other hand, the graph-based embeddings, while bad at capturing similarity perform better on the relatedness task. Moreover, they provide more insight into entity-entity relations and thus, work better for entity-centric tasks. In general, the usefulness of entity embeddings is very task dependent, and should not be used blindly in place of word embeddings, as normal word embeddings tend to obtain better when the datasets are purely term-based. Since the faceted models are applied to annotated text, they share many characteristics with the entity embeddings. Faceted models tend to favour relatedness over similarity and show promising results is entity-centric analogy tasks. However, dividing the embedding space by removing certain entity types during training has its drawbacks. The performance significantly degrades in comparison normal word embeddings on raw text, which we consider due to lack of information to learn from. To train each component. The vocabulary is limited to a certain entity type, which limits the context of the word and makes it more difficult for the model to learn a useful representation. Despite the decrease in performance, faceted models show promise for entity-centric search, where the individual components can be used to limit the search space to a specific type. \\
Considering the weaknesses and strength of all models, we see potential applications for entity embedding and faceted models in entity-centric tasks that benefit from relatedness relations, such as improved query expansion or learning to disambiguate, which we consider to be the most promising future research directions and downstream tasks.\\