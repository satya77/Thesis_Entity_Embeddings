%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}\label{chap:concl}

In this thesis, we investigated different techniques to jointly train embeddings for terms and entities on an annotated corpus with named entities. We considered the naive application of the popular models word2vec and GloVe to annotated texts, as well as embedding the nodes of co-occurrence graphs extracted from the annotated text. We \added{the} used popular node embedding methods DeepWalk and VERSE and \replaced{modified}{tweaked} them to \replaced{obtain}{achieve} the entity embeddings. Furthermore, to analyse the effect of each entity type on the embedding space, we explored variations of GloVe, word2vec, and DeepWalk that separate the embedding space into different embedding types. These faceted models consist of separable components, where each component indicates the relation of \replaced{a}{the} word to entities of a specific type. Moreover, we compared all the proposed models to traditional word embeddings on a comprehensive set of term-focused evaluation tasks and performed entity-centric exploration to identify the strengths and weaknesses of each approach. 
\section{Discussion}
We found\deleted{,} that even though training \deleted{the} state-of-the-art word embeddings directly on annotated text is possible, their performance degrades in term-centric tasks, \replaced{particularly}{in particular,} in tasks that depend on relatedness. \replaced{By contrast}{On the other hand}, the graph-based embeddings, while bad at capturing similarity\added{,} perform better on the relatedness task. Moreover, they provide more insights into entity-entity relations and thus, work better for entity-centric problems. Graph-based embeddings, in particular, show great promise for entity-centric analogy tasks but fail to form meaningful clusters of words\deleted{. In contrast,} \added{whereas} normal word embeddings are able to cluster similar words and even multi-word entities better and perform well on purely term-based analogy tasks. Exploration of entity neighbourhood\added{s} for each model shows that although normal word embeddings map the word with a similar meaning closer in the embedding space, as the neighbourhood shows only the synonymous or descriptive word.
% BR: The preceding sentence has an issue with grammar. Do you want to
% say that the proximity of words with similar meanings is better for
% regular word embeddings?
On the other hand, models on annotated text and graph-based embedding\added{s}, in particular, tend to map related and associated entities closer in embedding space and can be used for searching \deleted{for} related entities. In general, the usefulness of entity embeddings is very dependent on the problem at hand, and should not be used blindly in place of word embeddings.
\\
Since the faceted models use annotated text as input, they share many characteristics with the entity embeddings. Faceted models tend to favour relatedness over similarity and show promising results in entity-centric analogy tasks. However, dividing the embedding space by removing certain entity types during training has its drawbacks. The performance significantly degrades in comparison to normal word embeddings on raw text, which we consider due to a lack of information to learn from. To train each component, the vocabulary is limited to a certain entity type, which limits the context of the word and makes it more difficult for the model to learn a useful representation. The neural network-based approaches are data hungry and their performance is strongly correlated with the amount of available training data, thus, limiting the context of the word to a certain entity type decreases the training data and consequently also the performance.
% BR: Rephrase: 'Approaches based on neural networks, however, require
% larger amounts of data and their performance is strongly coupled to
% data availability. Hence, limiting the context of a word to a certain
% type of entity reduces the training data set, resulting in a lower
% performance.'
Perhaps with a more intelligent definition of context for these models, where the limitation of context does not limit the data for the model to learn from, we observe a boost in performance.
% BR: Rephrase: 'We hypothesize that with a more involved definition of
% context, ... we might observe observe'.
Despite the decrease in performance, faceted models show promise in entity-centric search, where the individual components can be used to limit the search space to a specific type. Moreover, similar to entity embedding\added{s}, they tend to map related entities closer together rather than synonymous words. Furthermore, by studying the different components of faceted models we found that surrounding terms are most significant when it comes to relatedness and similarity tasks, while for clustering similar entities the component corresponding to the entity type tends to produce better results.

\section{Future Work}
Although the comparison of the entity-based models with the word embedding on term-based datasets shines some light onto their differences and potential use-cases, true performance of these models can only be evaluated if entity-centric evaluation datasets are created.
% BR: Phrasing: 'sheds some light...'
As future work, creating a dataset that contains named entities for common intrinsic tasks for word embedding and designing specific intrinsic tasks for entity-centric evaluation can help future entity-based models to efficiently evaluate their methods.\\
% BR: replace 'can help' with 'will support the evaluation...'
Because of the ability of entity-based models to capture relatedness, a temporal analysis of the evolution of entity-entity relation\added{s} in a dataset with a temporal aspect, such as news streams, is possible. Similar approaches have been used to study the transition in meaning for a word over time, which can be used to investigate the evolution of entity-entity relations. Moreover, separate components of faceted embedding\added{s} allow for a more detailed study, where the relation of an entity to associated locations or actors can be analysed separately. \\
Overall, considering the weaknesses and strength of all models, we see potential applications for entity embeddings and faceted models in entity-centric tasks that benefit from relatedness relations, such as improved query expansion based on the related entities. Document ranking can also benefit from a vector representation of entity and words, especially when the document contains related entities to the query. Faceted models can be used to create more flexible search criteria based on entity types, which can be beneficial for entity ranking and search engines. Tasks that use named entities as part of their pipeline can also benefit from entity embeddings, such as\deleted{,} named entity recognition and disambiguation, which we consider to be the most promising future research directions and downstream tasks.\\
