%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}\label{chap:concl}
The weights of LOAD, similar to co-occurrence matrices, contain information about the statistics of the corpus and meaningful components can be extracted from them. In spite of this similarity using the weighted adjacency matrix for separate types of entities and then combining the results is unable to beat the state of the art models on common embedding tasks. The true potential of these embeddings might be visible in using them in a specific downstream task. As to what the dimensions really capture in these embeddings, not a meaningful explanation could be found in the visualizations. \\
One possible explanation for the relatively poor performance of the facetted model can be simply lack of information to learn from. For each type, the vocabulary is limited to a certain type and thus removing other surrounding words and limiting the context. \\

