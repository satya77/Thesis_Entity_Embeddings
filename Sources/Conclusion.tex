%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}\label{chap:concl}

In this thesis, we investigated different techniques to jointly train embeddings for terms and entities on an annotated corpus with named entities. We considered the naive application of the popular models word2vec and GloVe to annotated texts, as well as embedding the nodes of co-occurrence graphs extracted from the annotated text. We used popular node embedding methods DeepWalk and VERSE and tweaked them to achieve the entity embeddings. Furthermore, to analyse the effect of each entity type on the embedding space, we explored variations of GloVe, word2vec, and DeepWalk that separate the embedding space into different embedding types. These faceted models consist of separable components, where each component indicates the relation of the word to entities of a specific type. Moreover, we compared all the proposed models to traditional word embeddings on a comprehensive set of term-focused evaluation tasks and performed entity-centric exploration to identify the strengths and weaknesses of each approach. 
\section{Discussion}
We found, that even though training the state-of-the-art word embeddings directly on annotated text is possible, their performance degrades in term-centric tasks, in particular, in tasks that depend on relatedness. On the other hand, the graph-based embeddings, while bad at capturing similarity perform better on the relatedness task. Moreover, they provide more insight into entity-entity relations and thus, work better for entity-centric problems. Graph-based embeddings in particular, show great promise for entity-centric analogy task, but fail to form meaningful clusters of words. In contrast, normal word embeddings are able to cluster similar words and even multi-word entities better and perform well on purely term-based analogy task.Exploration of entity neighbourhood for each model shows that although normal word embeddings map the word with similar meaning closer in embedding space, as the neighbourhood shows only the synonymous or descriptive word. On the other hand, models on annotated text and graph-based embedding in particular, tend to map related and associated entities closer in embedding space and can be used for searching for related entities. In general, the usefulness of entity embeddings is very dependent on the problem at hand, and should not be used blindly in place of word embeddings.
  \\
Since the faceted models use annotated text as input, they share many characteristics with the entity embeddings. Faceted models tend to favour relatedness over similarity and show promising results is entity-centric analogy tasks. However, dividing the embedding space by removing certain entity types during training has its drawbacks. The performance significantly degrades in comparison to normal word embeddings on raw text, which we consider due to lack of information to learn from. To train each component, the vocabulary is limited to a certain entity type, which limits the context of the word and makes it more difficult for the model to learn a useful representation. The neural network based approaches are data hungry and their performance is strongly correlated with the amount of available training data, thus, limiting the context of the word to a certain entity type decreases the training data and consequently performance. Perhaps with a more intelligent definition of context for these models, where the limitation of context does not limit the data for the model to learn from, we observe a boost in performance. Despite the decrease in performance, faceted models show promise for entity-centric search, where the individual components can be used to limit the search space to a specific type. Moreover, similar to  entity embedding, they tend to map related entities closer together rather than synonymous words. 
\section{Future Work}
Considering the weaknesses and strength of all models, we see potential applications for entity embeddings and faceted models in entity-centric tasks that benefit from relatedness relations, such as improved query expansion based on the related entities. Named entity related tasks cann also benefit from such models. For example, named entity recognition and disambiguation, which we consider to be the most promising future research directions and downstream tasks.\\