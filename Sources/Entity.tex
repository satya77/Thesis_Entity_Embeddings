\chapter{Joint Entity and Word Embeddings}\label{chap:entity}
\begin{figure}
\centering 
\resizebox{0.97\textwidth}{0.32\textwidth}{      
\input{images/entity_emebddings_pipline.tex}
}
\caption{Pipeline for creating entity-based embeddings. Annotation of the raw text contains: POS tagging, entity recognition, and linkage. For textual models,Word embedding methods are applied to annotated text, while the nodes of a co-occurrence graph are embedded for the graph-based models.}
\label{fig:entity_emebddings_pipline}
\end{figure}
In this chapter, we explore different ways to jointly train word embeddings for entities and terms. We use the word and graph embedding on annotated data to generate such embeddings. In Section~\ref{sec:entity_overview}, an overview of all models is given and the objectives are explained. Section~\ref{sec:raw} is dedicated to models trained on unannotated (raw) text, that are used as the baseline against which we compare our models. The remaining two sections describe the two entity-based models proposed in this thesis. In Section~\ref{sec:annotated}, word embeddings that are trained on annotated text are explained, followed by graph-based models in Section~\ref{sec:graph_based}.
\section{Overview and Objectives}\label{sec:entity_overview}
We introduce entity-based embeddings to address the issues that entity-centric downstream tasks would face, when using word embeddings as features. Any downstream task that needs a vector representation of entities has to either additionally perform entity recognition and linkage on normal word embeddings, or settles to using the ambiguous features that such models present. To address this problem, we learn embeddings for entities and terms directly from the text, without any post-processing steps. The pipeline for generation of entity-based embeddings is shown in Figure~\ref{fig:entity_emebddings_pipline}.\\
For word and graph-based models, we annotate the corpus in the first step and use it directly as input, while the graph-based models have an additional step of co-occurrence graph extraction. The nodes of the graph are entities and terms in the text; thus, as an alternative method we embed the nodes of the co-occurrence graph to obtain embeddings for all the words in the vocabulary. \\
% BR: previous sentence is redundant: the embedding helps you get
% embeddings for all words; reformulate
Generating embeddings from annotated text is advantageous for the following reasons:
% BR: Rephrase: Generating embeddings from annotated text is
% advantageous for the following reasons:
\begin{compactitem}
\item \emph{Distinguishing homographs:} With annotation, we can distinguish between entity homographs, which written the same way but represent different entities. As a result, a vector representation of \emph{``Washington''} as a person is different from the same word as a location. \\
\item \emph{Embedding compound words:}  Unlike raw text, where the compound name is broken down into specific words, names of entities are grouped into a unique identifier that helps the model to learn a distinct representation for them. In traditional methods, to derive a unique representation for an entity such as \emph{``Donald Trump''}, embeddings for two parts of the word have to be averaged, multiplied or transformed in a separate post-processing step, which is not only inefficient but also misleading; as the name \emph{``Donald''} could refer to different people in a different contexts. Grouping the multi-word names from the starts helps avoid such ambiguous representations.\\ 
\item \emph{Unifying all surface form of an entity:} Mentions of the same entity with different surface forms are considered a unit identity, e.g., \emph{``Obama''} and \emph{``Mr. President''} in the time frame after $2009$. This approach lead to more  word context pair for each entity, which can increase performance. For example, all the words surrounding \emph{``Obama''} or any other surface form of the entity are considered as context words for \emph{``Barack Obama''}. \\
\item \emph{Unique representation of temporal information:} We identify expressions that are dates or time, and normalise them into a single unique date. Hence, we can incorporate the temporal information as the context of a word, and at the same time learn a vector representation for the dates, which is missing from the traditional word embeddings. \\
\item \emph{Customizable search:} Since entities have pre-defined types, such as actor or location, we can perform a more effective search in the embedding space. The neighbourhood of a word can be filtered to contain only relevant types, which is beneficial for finding relationships between people or organizations. \\
\item \emph{Flexible neighbourhood definition:} In a term-based model, a word is only defined by the terms surrounding it.  Annotated named entities, however, permit different definitions of a word, based on the type of entities that surround it.
% BR: Rephrase: Annotated named entities, however, permit different
% definitions of a word, based on the type of entities that surround it.
For example, when looking for the nearest neighbours of a word, the neighbourhood can be filtered to contain only entities of a specific type or remove the entities altogether and look only at surrounding terms. We take advantage of this property in Chapter~\ref{chap:eval} and introduce a new search criterion that succeeds in capturing entity-entity relations better than term-based models.\\
\end{compactitem}
In general, removing ambiguous mentions and annotating the corpus provides the models with more information and is likely to result in better input features for entity related tasks, such as information retrieval~\brackettext{\cite{DBLP:conf/acl/0001MC16,DBLP:conf/cikm/KuziSK16}}, and named entity recognition~\brackettext{\cite{DBLP:conf/nodalida/Siencnik15}}.
Since there is no framework available that uses such embeddings as an input, the claim for better performance in any downstream task can only be validated if such frameworks are created. However, in this thesis, the focus is not on constructing task-specific features for a downstream task, but rather to create a general-purpose enity-based embedding model that is comparable to normal word embeddings. As a result, we focus only on creating entity embeddings that can perform similarly or even better than normal word embeddings on common term-based evaluation tasks.\\
In the remainder of this chapter, we introduce methods for jointly training entity and term embeddings on an annotated corpus. The main objective of entity-based models is to upgrade the classical word embeddings by incorporating the type of the word, and also to learn a unique representation for named entities mentioned in the text. We focus on learning these embeddings by tweaking the existing word and graph embedding techniques and evaluating which one works best. For this purpose, we propose two approaches: 
\begin{compactenum}
\item To naively include entities in our models, we train the well-established word embedding models on a corpus, annotated with named entities. 
\item To better capture the entity-entity relations, we take advantage of the graph representation of the corpus, and embed the nodes of co-occurrence graphs extracted from the annotated text. 
\end{compactenum}
To enhance the performance of our models, we try a wide range of word and graph embedding  techniques and compare them against word embedding models trained on raw text.

\section{Word Embeddings on Raw Text}\label{sec:raw}
State-of-the-art word embedding models are trained on raw text (without entity annotation). The word2vec and GloVe models, explained in Chapter~\ref{chap:background}, Sections~\ref{subsec:word2vec} and~\ref{subsec:GloVe} respectively, are chosen as representatives of classical word embeddings. From the word2vec model, the skip-gram architecture is chosen. We refer to the word2vec and GloVe model as $r$W2V and  $r$GLV, where \emph{``r''} denotes raw text input. The text is cleaned only by common pre-processing steps, such as tokenization and stop word removal. The complete pre-processing steps is explained in Chapter~\ref{chap:eval}, Section~\ref{sec:data}, where we discuss the training data. 
% BR: can you briefly mention some of the pre-processing steps?
% Otherwise, this is slightly vague. Or does it come afterwards? I shall
% see!
Since no entities are annotated, all words are treated as terms. These models are used later in Chapter~\ref{chap:eval} to assess the performance of models proposed in this study. 

\section{Word Embeddings on Annotated Text}\label{sec:annotated}
\begin{figure}
\centering 
\resizebox{0.85\textwidth}{0.28\textwidth}{      
\input{images/annotation.tex}
}
\caption{An example of the pre-processing for annotated text. All the entity mentions are replaced with their unique identifier and the unnecessary POS-tags are removed.}
\label{fig:annotation}
\end{figure}
Named entities $N \subseteq V$ are a subset of all words in the vocabulary that fall into a pre-defined category.
% BR: change this to: Named entities $N \subseteq V$ are a subset of all...
In other words, words that have a specific type are considered as named entities. Hence, the simplest way to obtain their embeddings would be to run the well-established word embedding models on the annotated text, where these named entities are identified and annotated. Our contribution to these models is not the modification of the underlying method, but the input. The input of the model is no longer the raw text, but a corpus with annotated named entities. Annotation involves POS tagging, named entity recognition and disambiguation, which are performed using the state-of-the art tools on raw text.
% BR: Would rather use 'Task of annotating them (?)' here or just say
% 'Annotation involves...'
Since word embeddings learn the representation for each token in the text, our definition of the token is changed after annotation. A token is no longer a single word separated by a space but a named entity with a specific type. Any word that does not fit into any of the pre-defined types is considered as a term. Unlike the set of all words $V=N \bigcup T $, in which multiple entities can share a single ambiguous surface form, entities are presented with their unique identifiers. In our case, the identifier is the type of the word and numeric key, e.g., \emph{ACT\_17}, for an actor. As a result, compound words, such as names of people and organisations, are considered as one word. For example, all mentions of \emph{A\_17}, which corresponds to the actor \emph{``Barack Obama''} are represented with a single identifier.
% BR: Do you mean 'identifier' rather than 'notation'?
This process makes the text less ambiguous.
% BR: Fragment; change to 'This makes the...'
Furthermore, depending on the performance of the named entity recognition tool all other indirect mentions, e.g., \emph{``Mr. President''}, \emph{``President of the United States''}, are also replaced with their corresponding entity identifier, resulting in more context pair for a focal word.
% BR: No, the identifier is an element from that set rather than the
% whole of the set, right?($V\setminus  N$)
The same applies to different surface form of dates, as all the dates in their different surface forms  are normalized to a single form. Moreover, words that belong to more than one entity type, have an embedding for each type. For example, \emph{``Washington''} can either be a name of a person or a city and has two separate embeddings , one for the actor and on for the location context. The same naming strategy applies for terms, e.g., \emph{TER\_22}, where all the unique mentions of terms in text are identified and replaced with their corresponding identifier. By adding the type information in the name of the identifier, we can group named entities of the same type and also to search for a specific type. \\
Since entity annotation requires POS tagging, we used the POS tags to remove stop-words and punctuation. An example of the text after annotation and pre-processing is shown in Figure~\ref{fig:annotation}. After annotation, we use word2vec and GloVe and refer to them as $a$W2V and $a$GLV, where \emph{``a''} denotes the use of annotated text. 

\section{Node Embeddings of Co-occurrence Graphs}\label{sec:graph_based}

One motivation for incorporating the graph-based methods to enhance the performance is the additional information that such graphs contain. Most word embedding methods consider a fixed sized window of few words to determine if two words are related to each other. Named entities, however, have more complex relational structures. An actor and an organization can be related even if they are mentioned several sentences apart. An example text is shown in Figure~\ref{fig:article_entities}, where possible relations between two actor entities, \emph{``Donald Trump''} and \emph{``Fred C. Trump''} are shown. If the context window is limited to a few words around the center word, then the relationship would not be detected, except for a window size of over $53$ tokens. As a result, word-based windows are unable to detect certain relationships that go beyond a sentence limit. 
% BR: Rephrase: 'the relationship would not be detected, except for
% a window size of over 53 tokens' or something like that. I believe
% you are *unable* to detect the relationship using word-based windows,
% right?
On the other hand, if we look at the sentence distance of even one sentence away, \emph{``Donald Trump''} and \emph{``Fred C. Trump''} would be considered related. This is one of the reasons behind the choice of the LOAD model as a co-occurrence graph. The LOAD model applies a sentence-based window to find entity relations and a word-based window for relations among terms, and thus, captures entity-to-entity relations better than a word-based window.

\begin{figure}
\centering 
\resizebox{0.90\textwidth}{0.2\textwidth}{      
\input{images/article_entities.tex}
}
\caption{An example text to show the relation between two actor entities, \emph{``Donald Trump''} and \emph{``Fred C. Trump''}. If the word-based window size is applied then the relationship between the two entities is not captured, even though they are only one sentence apart.}
\label{fig:article_entities}
\end{figure}
\noindent
We defined a co-occurrence graph of words in Chapter~\ref{chap:background}, as a graph $G=(V,E)$, where $V$ is the set of words in the vocabulary (nodes) and $E$ is the set of edges. Furthermore, we assume that there are edge weights that encode some notion of distance between the words.
% BR: Technically, your definition does not define edge weights. So you
% could add 'Furthermore, we assume that there are edge weights that
% encode some notion of distance...'
We extract such a graph from the textual corpus with named entity annotations. As a result, we obtain a weighted heterogeneous graph $G=(N\bigcup  \overline{N},E)$ of named entities and terms, where the set $N$ contains entities of different types and $ T=\overline{N}$ denotes all the words that are not a named entity (terms).
% BR: I do not like the choice of symbol; if you want to express the
% set-theoretic complement of $N$, use $\complement$.
For a text with entity annotations in particular, graphs implicitly extracted from text, such as LOAD, can serve as graph representations of the corpus. By using a sentence-window for finding the relationship between entities, they can capture entity relations even sentences apart, while applying a word-based distance for terms. Since these graph represenations capture the relations of all terms and entities in the text, by embedding the nodes, we can obtain word embeddings for both. We later show that graph-based methods achieve a better performance in comparison to $a$W2V and $a$GLV on the annotated text. For embedding nodes, we chose DeepWalk (DW)~\brackettext{\cite{SCHOL:website/Perozzi2014}} and VERSE (VRS)~\brackettext{\cite{SCHOL:website/Tsitsulin2018}}, explained in Chapter~\ref{chap:background}, Sections~\ref{subsec:DeepWalk} and ~\ref{subsec:VERSE}, respectively.\\
Since we work on weighted graphs and weights display the strength of the relationships, we modify the DeepWalk model to take edge weights into account. For this reason, \emph{weighted random walks} are introduced to replace the uniform random walk in the original DeepWalk model.
% BR: Start the next sentence with something like 'In such a weighted
% random walk...'
In such a weighted random walk, the probability of a node visiting its neighbours is proportional to the edge weight, where the more stronger the relationship, the more likely a neighbouring node is to be visited. Hence, the probability of visiting node $j$ from vertex $i$ with edge weight $e_{i,j}$, where $ E_{ i }$ denotes the set of all edges starting from node $v_{i}$, is given in Equation~\ref{eq:edge_weight}. $f$ is a normalization function that we will subsequently introduce.
\begin{equation}
P_{i,j}=\frac{f(e_{i,j})}{\sum _{ f(e_ k )\in E_{ i } }^{  }{ f(e_k) } }
\label{eq:edge_weight}
\end{equation}

% BR: ...the more likely a node is to be visited
% BR: add: ...so we obtain <EQ>.
% BR: How is $f$ defined? I know it comes later, but I would rather say
% '...where $f$ is a normalization function, which we will subsequently
% introduce'

\begin{figure}
\centering 
\resizebox{0.6\textwidth}{0.6\textwidth}{      
\input{images/weights.tex}
}
\caption{Weighted degree distribution of the LOAD network.}
\label{fig:degrees}
\end{figure}
\noindent
In a word co-occurrence graph, some words co-occur many times and some only a few times. Therefore, the weights can fluctuate between small numbers up to tens of thousands, resulting in highly unbalanced transition probabilities, causing some nodes to never be visited in a random walk. The weighted degree distribution for LOAD graph, extracted from our training corpus, as an example of such unbalanced distribution, is shown in Figure~\ref{fig:degrees}. The unbalanced distribution may cause the random walker to focus on certain nodes, while ignoring the others, in other words, the edge that have a large weight would be overemphasized, which can lead to poor representation of nodes with smaller weights. To solve this issue and create a more balanced weight distribution, we use different normalization functions on the edge weights:
\noindent
\begin{enumerate}
\item  $f=\mathrm{id}$, identity function, where the weights remain unchanged. We refer to these models as DW$_{id}$, where $id$ refers to the identity mapping.
\item  $f=\log$, logarithmic normalization, where all edge weights are replaced by their logarithm and the transition probabilities are computed accordingly. These models are denoted by DW$_{log}$.
% BR: I would state this non-result afterwards: 'Initially, we also
% experimented with the square root, but we found that is produces
% similar results to the logarithmic normalization, so ...'
\end{enumerate}
Initially, we also experimented with the square root ($f=\mathrm{sqr}$), but we found that is it produces similar results to the logarithmic normalization, so it is removed from the results.\\
% BR: This drops out of nowhere. Can you briefly reference what you
% refer to? Also, this belongs into a different paragraph because it
% does not deal with the random walks anymore.

% BR: I would rephrase this: 'We could not use the LINE model for the
% following reasons...'. The current phrasing is very informal.
\begin{figure}
\centering 
\resizebox{0.50\textwidth}{0.35\textwidth}{      
\input{images/simrank.tex}
}
\caption{An example of SimRank similarity. Although the relationship between the president of Iran and the U.S. is discovered by SimRank, the more important relationships, namely direct neighbours are disregarded. The similarity between \emph{``Barack Obama''} and its direct neighbours are zero.}
\label{fig:simrank}
\end{figure}

% BR: This requires some details or back-references if you already
% talked about the model. How is the Adjacency Similarity defined, for
% example?
Our next candidate for generating node embedding is the VERSE model. Unfortunately, The VERSE model does not support the incorporation of the edge weights, and the edge weights are disregard for this model. From the three similarity measures that VERSE offers, we chose Adjacency Similarity, explained in Chapter~\ref{chap:background} in Section~\ref{subsec:VERSE}, to produce the entity embeddings. Our reason is two-fold: first, the PPR, explained in Chapter~\ref{chap:background} in Section~\ref{subsec:VERSE}, relates to the stationary distribution of a random walk with restart and we already represented the random walk-based models with DeepWalk.
% BR: Where does the PPR drop from? Give a quick refresher here and
% refer to the appropriate section in your document.
Second, SimRank, explained in Chapter~\ref{chap:background} in Section~\ref{subsec:VERSE}, is a measure of structural relatedness, between two nodes, based on the assumption that two nodes are similar if they are connected to other similar nodes. Although words that co-occur often with the same words can themselves be considered similar, if we only look at nodes that have common neighbours we lose most of the adjacency relations in the graph. An example is illustrated in Figure~\ref{fig:simrank}. Since the SimRank of two nodes with no common neighbour is zero, the direct neighbours are disregarded. However, SimRank tends to discover interesting relationships, such as the one shown in Figure~\ref{fig:simrank} and possibly if a new similarity measure is introduced that creates a trade-off between SimRank and Adjacency Similarity, the results might improve. For this study, however, we choose only Adjacency Similarity to learn the embeddings and we leave the development of a new similarity measure as future work. 
% BR: ...and we leave the development of a new similarity measure as
% future work
Since Adjacency Similarity is based on the normalized adjacency matrix, it captures the neighbourhood relations and focuses on the direct neighbours of a node.\\
% BR: Rephrase: Since it [i.e. Adjacency Similarity] is based on the
% normalized adjacency matrix, ...
The other two embedding methods explained in Chapter~\ref{chap:background} are not used to generate entity-based embeddings. We could not use the LINE model for the following reasons : First, because of the unbalanced weight distribution, the optimisation objective for the second-order proximity of the LINE model becomes ill-defined. Hence, one part of the embedding for this model cannot be trained properly. Second, the Adjacency Similarity of VERSE model correlates with the first-order proximity in LINE and can be used as representative of first-order proximity. The node2vec model is quite slow and inefficient for large graphs, consequently, because of the time and memory overhead fora co-occurrence graph extracted from a large corpus of text, we did not use this method.  \\
% BR: Same issue: SimRank appears to drop out of nowhere and needs to be
% put into context.
\section{Summary}\label{sec:entity_summary}
In this chapter, we introduced two methods to extract entity and term embeddings from an annotated corpus. One method learns embeddings directly from the text, annotated using state-of-the-art named entity recognition and disambiguation tools. While the other generates a co-occurrence graph for the annotated corpus and embeds its nodes. In the next chapter, we attempt to create embeddings separable by entity type, namely faceted embeddings, and in in Chapter~\ref{chap:eval}, we report the evaluation results for both entity-based models and faceted embeddings. Furthermore, we show why the extraction of a co-occurrence graph is crucial for better performance. 
% BR: If you developed this by yourself, you can give yourself more
% credit :-]
