\chapter{Entity Embeddings}\label{chap:entity}

In this chapter, we try to explore different ways to train word embeddings for entities and terms jointly. We use the word embedding and graph embedding models explained in the previous chapter, on annotated data to generate such embeddings. In Section~\ref{sec:entity_overview}, an overview of all models is given and the objectives are explained. Section~\ref{sec:raw} is dedicated to models trained on unannotated (raw) text, that are used as the baseline against which we compare our models. The remaining two sections describe the two entity-based models proposed in this thesis. In Section~\ref{sec:annotated},  word embeddings that are trained on annotated text are explained, followed by graph-based models in Section~\ref{sec:graph_based}. Finally, we explain how to measure the similarity between two embeddings in Section~\ref{sec:similarity}
\ornament
We introduce entity-based embeddings to address the issues that entity-centric downstream tasks would face when using word embeddings as features. An annotated corpus has many benefits. To name a few: 
\emph{Distinguishing homographs:} with annotation, we can distinguish between entity homographs. As a result, a vector representation of \emph{``Washington''} as a person is different from the same word as location. \\
\emph{Embedding compound words:}  unlike raw text, where the compound name is broken down into specific parts, names of entities are grouped into a unique identifier that helps the model to learn a distinct representation for it.  In traditional methods, to derive a unique representation for an entity like \emph{``Donald Trump"}, embeddings for two parts of the word have to be averaged, multiplied or transformed in some post-processing step, which is not only inefficient but also misleading; as the name \emph{``Donald"} can refer to many other \emph{``Donalds"} in a different contexts. \\
\emph{Unifying all surface form of an entity:} Mentions of the same entity with different surface forms are considered a unit identity, e.g., \emph{``Obama''} and \emph{``Mr. President''} in the time frame after $2009$, which creates a word context pair for each entity, which can increase performance. For example, all the words surrounding \emph{``Obama''} or any of aliases are considered as context word for the entity \emph{``Barack Obama''}. \\
We identify expressions which are dates or time, and normalise them into a single unique date. Hence, we can incorporate the temporal information as the context of a word, and at the same time learn a vector representation for the dates. \\
\emph{Customizable search:} Since entities have pre-defined types, such as actor or location, we can perform a more effective search in the embedding space. The neighbourhood of a word can be filtered to contain only relevant types, which is beneficial for finding relationships between people or organizations. \\
\emph{Flexible neighbourhood definition:} In a term-based model, a word is only defined by the terms surrounding it. With annotated named entities now we can have different definitions of a word, based on the type of entities surrounding it. A word can be defined by the most relevant locations or actors. For example, when looking for the nearest neighbour of a word, the neighbourhood can be filtered to contain only entities of a specific type or remove the entities altogether and look only at surrounding terms. We take advantage of this property in Chapter~\ref{chap:eval} and introduce a new search criterion that succeeds in capturing entity-entity relations better than term-based models. \\

In general, removing ambiguous mentions and annotating the corpus provides the models with more information and is stand to produce better input features for entity related tasks, such as information retrieval~\brackettext{\cite{DBLP:conf/acl/0001MC16,DBLP:conf/cikm/KuziSK16}}, named entity recognition~\brackettext{\cite{DBLP:conf/nodalida/Siencnik15}}. Since the concept of embedding named entities has not been studied until now, there is no framework available that uses such embeddings as an input. Consequently, the claim for better performance in any downstream task can only be validated if such frameworks are created. As a result, in this thesis, we only focus on creating entity embeddings that can perform similar or better to normal word embeddings on the common term-based evaluation tasks.
\section{Overview and Objectives}\label{sec:entity_overview}
In this chapter, we introduce methods for training entity and term embeddings jointly, on an annotated corpus.The main objective of entity-based models is to upgrade the traditional term embeddings, by incorporating the type of the word and also to learn a unique representation for named entities mentioned in the text. We focus on learning these embeddings by tweaking the existing word and graph embedding techniques and evaluating which one works best. For this purpose, propose two approaches: 
\begin{compactenum}
\item To naively include entities in the embeddings, we train the well-established word embedding models on a corpus, annotated with named entities. 
\item Embedding the nodes of co-occurrence graphs extracted from the annotated text. 
\end{compactenum}
Both of the models are compared against word embedding models trained on raw text. To achieve better performance, we try a wide range of different techniques, starting from the well-established word embeddings methods on annotated corpus to graph embedding techniques on word co-occurrence graphs. As described later in the Chapter~\ref{chap:eval}, the first method produces poor results on our evaluation tasks. Hence, the graph-based models are proposed to solve this problem. 
\section{Word Embeddings on Raw Text}\label{sec:raw}
State-of-the-art word embedding models are trained on raw text (without entity annotation). The word2vec and GloVe models, explained in Chapter~\ref{chap:background}, Sections~\ref{subsec:word2vec} and~\ref{subsec:GloVe} respectively, are chosen as representative of classical word embeddings. From the word2vec model, the skip-gram architecture is chosen. We refer to the word2vec and GloVe model as $r$W2V and  $r$GLV, where \emph{``r"} denotes raw text input. The text is cleaned only by common pre-processing steps.  Since no entities are annotated, all words are treated as terms. 
\section{Word Embeddings on Annotated Text}\label{sec:annotated}
\begin{figure}
\centering 
\resizebox{0.85\textwidth}{0.28\textwidth}{      
\input{images/annotation.tex}
}
\caption{An example of the pre-processing for annotated text. All the entity mentions are replaced with their unique identifier and the unnecessary tags POS-tags are removed. }
\label{fig:annotation}
\end{figure}
Named entities $N$ are a subset of all terms in the vocabulary $N\subseteq T$ that fall into a pre-defined category. In other words, terms that have a specific type are considered as named entities. Hence, the simplest way to obtain their embeddings would be to run the well-established word embedding models on the annotated text. Our contribution to these models is not the modification of the underlying method, but the input. The input of the model is no longer the raw text, but a corpus with annotated named entities. The task of annotation involves POS tagging, named entity recognition and disambiguation. Since word embeddings learn the representation for each token in the text, our definition of the token is changed. A token is no longer a single word separated by space but named entity with a specific type. Any word that does not fit into any of the pre-defined types is considered as a term. Unlike the set of all terms $T$, in which multiple entities share can share single ambiguous surface form, entities are presented with their unique identifiers. In our case, the identifier is the type of the word and numeric key, e.g,  \emph{ACT\_17}, for an actor. As a result, compound words, such as names of people and organizations are considered as one word. All mentions of  \emph{A\_17}, which corresponds to the actor  \emph{``Barack Obama"} are represented with a single notation. Thus, making the text less ambiguous. In addition, depending on the performance of the named entity recognition tool all other indirect mentions, e.g. \emph{``Mr. President"}, \emph{``President of the United States"}, are also replaced with their corresponding entity identifier ($T\setminus  N$). The same applies to different mentions of dates that result into date embeddings. Words that belong to more than one entity type, have an embedding for each type. For example, \emph{``Washington"} can either be a name of a person or a city and will have two separate embeddings for the actor and location context. The same strategy applies for terms, e.g,  \emph{TER\_22}.  Adding the type information in the name helps us identify and group named entities easier. It also allows us to search for a specific type. \\
Since entity annotation requires POS tagging, we used the POS tags to remove stop-word, punctuation.An example of the text after annotation and pre-processing is shown in Figure~\ref{fig:annotation}. After annotation, we use word2vec and GloVe and refer to them as $a$W2V and $a$GLV, where \emph{``a"} denotes the use of annotated text. 

\section{Node Embeddings of Cooccurrence Graphs}\label{sec:graph_based}


One motivation for incorporating the graph-based methods to enhance the performance was because of the additional information that such graphs contain. Most word embedding methods consider a fixed sized window of few words to determine if two terms are related to each other. Named entities, however, have more complex relational structures. An actor and an organization can be related even if they are mentioned several sentences apart. An example text is shown in Figure~\ref{fig:article_entities}, where possible relations between  two actor entities, \emph{``Donald Trump"} and \emph{``Fred C. Trump"} is shown. If the context window is limited to a few words around the center word, then the relationship would remain detected, or else a window size of over $53$ tokens is required. On the other hand, if we look at the sentence distance of even one sentence away,  \emph{``Donald Trump"} and \emph{``Fred C. Trump"} would be considered related. This is one of the reasons behind the choice of LOAD~\brackettext{\cite{DBLP:conf/sigir/SpitzG16}} model as a co-occurrence network. The LOAD model applies a sentence-based window to find entity relations and a word-based window for relations among terms, and thus, captures entity-to-entity relations better than a fixed window.

\begin{figure}
\centering 
\resizebox{0.90\textwidth}{0.2\textwidth}{      
\input{images/article_entities.tex}
}
\caption{An example text to show the relation between two actor entities, \emph{``Donald Trump"} and \emph{``Fred C. Trump"}. If the word-based window size is applied then the relationships between the two entities is not captured, even though they are only one sentence apart. }
\label{fig:article_entities}
\end{figure}
\noindent
We defined a co-occurrence graph of words in Chapter~\ref{chap:background}, as a graph $G=(T,E)$, where $T$ is the set of terms in the vocabulary as nodes and $E$ set of edges and the edge weights encode some notion of distance between the words. We extract such graph from the textual corpus with named entity annotations. As a result, we obtain a weighted heterogeneous graph $G=(N\bigcup  \acute {N},E)$ of named entities and terms, where the set $N$ contains entities of different types and $ \acute {N }$ denotes all the words which are not a named entity (terms). For entity annotations in particular, co-occurence graphs, which are implicitely extracte from text, such as LOAD, can serve as graph representations. By using stenece-window for finding the relationship between entities, they can capture entity relations even stentences apart. ~\brackettext{\cite{DBLP:conf/sigir/SpitzG16}}. Thus, by embedding the nodes, we can obtain word embeddings for both terms and entities.  We later show that this method achieves a better performance in comparison to $a$W2V and $a$GLV on the annotated text. For the node embeddings, we chose DeepWalk (DW)~\footnote{Our model is an adoption of the DeepWalk package in python: https://github.com/phanein/deepwalk} and VERSE (VRS)~\footnote{VERSE package is available in C++: https://github.com/xgfs/verse}, explained in Chapter~\ref{chap:background}, Sections~\ref{subsec:DeepWalk} and ~\ref{subsec:VERSE} respectively. However, since we work on weighted graphs and weights display the strength of the relationships, weighted random walks are introduced to replace the uniform random walk in the DeepWalk model. The probability of a node visiting its neighbors is proportional to the edge weight. As a result, the probability of visiting node $j$ from vertex $i$ with edge weight $e_{i,j}$,  is given in Equation~\ref{eq:edge_weight}, where $ E_{ i }$ denotes the set of all edges starting from node $t_{i}$. The stronger the relationship, the more probable is it to be visited. The function $f$ is our \emph{normalization function}. 
\begin{equation}
P_{i,j}=\frac{f(e_{i,j})}{\sum _{ f(e_ k )\in E_{ i } }^{  }{ f(e_k) } }
\label{eq:edge_weight}
\end{equation}
In a word co-occurrence
graph, some words co-occur many times and some only a few times. Therefore, the weights can fluctuate between small numbers up to tens of thousands, resulting in highly unbalanced transition probabilities, causing some nodes to never be visited in a random walk. The weighted degree distribution for LOAD graph as an example is shown is Figure~\ref{fig:degrees}. To solve this issue and create a more balanced weight distribution, we use different normalization functions:

\begin{figure}
\centering 
\resizebox{0.6\textwidth}{0.6\textwidth}{      
\input{images/weights.tex}
}
\caption{Weighted degree distribution of the LOAD network. }
\label{fig:degrees}
\end{figure}
\noindent
\begin{inparaenum}
\item  $f=\mathrm{id}$, identity function, where the weights remain unchanged. We refer to these models as DW$_{id}$, where $id$ shows the identity mapping. \\
\item  $f=\log$, logarithmic normalization, where all edge weights are replaced by their logarithm and the transition probabilities are computed accordingly. These models are denoted by  DW$_{log}$.\\
\item  $f=\mathrm{sqr}$, square root of each edge weight is computed. But since it produces similar results to logarithmic normalization, it is removed from the results.\\
\end{inparaenum}
\noindent
Because of the unbalanced weight distribution, the optimization objective for the second-order proximity of LINE model becomes ill-defined. Hence, one part of the embedding for this model cannot be trained properly and since only half an embedding does not represent the model, LINE is not used in this study. \\
\begin{figure}
\centering 
\resizebox{0.50\textwidth}{0.35\textwidth}{      
\input{images/simrank.tex}
}
\caption{An example of SimRank similarity. In this graph, although the relationship between the president of Iran and the US is discovered by SimRank, the more important relationships, namely direct neighbours are disregarded. The similarity between \emph{``Barack Obama"} and the direct neighbours are zero.  }
\label{fig:simrank}
\end{figure}
\noindent
The VERSE model does not support the incorporation of the edge weights. From the three similarity measure that VERSE offers, we chose Adjacency Similarity to produce the entity embeddings. Our reason is two-folded: first, the PPR relates to the stationary distribution of a random walk with restart and we already represented the random walk-based models with DeepWalk. Second, SimRank is a measure of structural relatedness, between two nodes, based on the assumption that two nodes are
similar if they are connected to other similar nodes. Although words that happen often with the same words can themselves be considered similar, if we only look at nodes that have common neighbours we lose most of the adjacency relations in the graph. An example is illustrated in Figure~\ref{fig:simrank}. Since the SimRank of two nodes with no common neighbour, is zero, the direct neighbour are disregarded. However, SimRank tends to discover interesting relationship, such as the one shown in Figure~\ref{fig:simrank} and possibly if a new similarity measure is introduced that creates a trade-off between SimRank and Adjacency Similarity, the results might improve. For this study, however, we choose only Adjacency Similarity to learn the embeddings. Adjacency Similarity is based on the normalized adjacency matrix and captures the neighborhood relations. Moreover, it correlates with the first-order proximity in LINE and can be used as representative of first-order proximity. \\
The general pipeline for generation of entity-based embeddings is shown in Figure~\ref{fig:entity_emebddings_pipline}. Word and graph-based models the first steps of annotation, where the graph-based models have an additional step of co-occurrence graph extraction. Nodes of the graph are the named entities and terms in the text, thus, through embedding the nodes we achieve similar results. 
\begin{figure}
\centering 
\resizebox{0.97\textwidth}{0.32\textwidth}{      
\input{images/entity_emebddings_pipline.tex}
}
\caption{Pipeline for generating entity-based embeddings. The first step is the annotation of the raw text with POS tagging, entity recognition, and disambiguation. Word embedding methods are applied to annotated corpus to create the textual models. A co-occurrence graph is extracted and nodes are embedded to achieve the graph-based models.   }
\label{fig:entity_emebddings_pipline}
\end{figure}
\section{Similarity Between Embeddings }\label{sec:similarity}
The dense word vector is learned to capture the semantics of the words and hence similar words are close in the induced space. \emph{cosine similarity} helps to capture this semantic closeness and is the most common similarity measure for the word vector. The cosine similarity is a measure that calculates the cosine of the angle between two vectors. This metric is a measurement of orientation and not magnitude and it is derived from the equation of a dot product between two vectors (Equation~\ref{eq:cosine}). The vectors are normalized by their length, which removes the influence of their magnitude on the similarity. The norm of the vector is somewhat related to the overall frequency of which words occur in the training corpus, but the direction is unaffected by this. So in order for a common word like \emph{``frog"} to still be similar to a less frequent word like \emph{``Anura"} (a type of frog), cosine distance which only looks at the direction works better than simple Euclidean distance. Moreover, cosine similarity is symmetric and changing the order of vectors in the dot product does not affect the final result.
\begin{equation}
\begin{split}
\overrightarrow { w_i } .\overrightarrow { w_j } =\parallel \overrightarrow { w_i } \parallel \parallel \overrightarrow { w_j } \parallel cos\theta 
\\
cos\theta =\frac { \overrightarrow { w_i } .\overrightarrow { w_j }  }{ \parallel \overrightarrow { w_i } \parallel \parallel \overrightarrow { w_j } \parallel  } 
\end{split}
\label{eq:cosine}
\end{equation}
\ornament
In this chapter,  two methods to extract entity and term embeddings from an annotated corpus were introduced. One method learns the embeddings directly from the text, while the other one generates a co-occurrence graph and embeds the nodes. In the next Chapter we attempt to create embeddings separable by entity type and in In Chapter~\ref{chap:eval}, we report the evaluation results for both entity based and facetted embeddings. We also show why the extraction of a co-occurrence graph is crucial for achieving better performance. 

%---- these parts have to be moved to the data section : 
%The corpus is tokenized into words and stop words and punctuations are removed. Non-alphabetic characters, like Chinese or Arabic vocabulary, are disregard and multiple white spaces are reduced to only one.  Any HTML tags or numeric values are also removed from the text, to produce a clean document, containing only terms relevant to the model. In addition, all the words are converted to their lower-case form to avoid multiple representations of a single term.


%
%For text pre-processing, punctuation and stop-word are removed and terms in the text are filtered to a certain POS-tag. Unnecessary tags are removed, namely  wh-determiner, wh-pronouns, wh-adverbs, common verbs, such as  \emph{have} and  \emph{do} in past or present, interjections (e.g. ah!, dear me!) and other terms that mostly fall into stop-words category such as: predeterminer (both and a lot of), possessive endings and prepositions (until, before,...), determiner (a, the, every) and coordinating conjunction (and, but, or). 
