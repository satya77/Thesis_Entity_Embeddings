\chapter{Enity Embeddigns}\label{chap:entity}

In this chapter, we try to explore different ways to train word embeddings for entities and terms jointly. We use the word embedding and graph embedding models explained in the previous chapter, on annotated data to generate such embeddings. In Section~\ref{sec:entity_overview}, an overview of all models is given and the objectives are explained. Section~\ref{sec:raw} is dedicated to models trained on unannotated (raw) text, that are used as the baseline against which we compare our models. The remaining two sections describe the two entity-based models proposed in this thesis. In Section~\ref{sec:annotated},  textual word embeddings that are trained on annotated text are explained, followed by graph-based models in Section~\ref{sec:graph_based}. Finally, we explain how to measure the similarity between two embeddings in Section~\ref{sec:similarity}

\section{Overview and Objectives}\label{sec:entity_overview}
Entity embeddings are word embeddings that are deduced from an annotated corpus of text. In this thesis, we focus on learning entity embeddings by tweaking the existing word and graph embedding techniques and evaluating which one works best. Therefore, we try a wide range of different techniques, starting from the well-established word embeddings methods on a corpus that contains named entity annotations, to graph embedding techniques on word co-occurence networks. As described later in the Chapter~\ref{chap:eval}, the first method produces poor reults on our evaluation tasks. Hence, the graph-based models are proposed to solve this problem. The main objective of entity-based models is to upgrade the traditional term embeddings, by incorporating the type of the word and also to learn a unique representation for named entities mentioned in the text. In traditional methods, to derive a unique representation for an entity like \emph{``Donald Trump"}, embeddings for two parts of the word have to be averaged, multiplied or transformed in some post-processing step, which is not only inefficient but also misleading; as the name \emph{``Donald"} can refer to many other \emph{``Donalds"} in a different contexts. Therefore, learning the embedding for the combination of these compound words from the start, results removes this problem. Moreover, entity related tasks that use these embeddings as input features, such as information retrieval~\brackettext{\cite{DBLP:conf/acl/0001MC16,DBLP:conf/cikm/KuziSK16}}, named entity recognition~\brackettext{\cite{DBLP:conf/nodalida/Siencnik15}} would have a better features, which leads to better performance. Since the concept of embedding named entities has not been studied until now, there is no framework available that uses such embeddings as an input. Consequently, the claim for better performance in any downstream task can only be validated if such frameworks are created. As a result, in this thesis we only focus on creating entity embeddings that can perform similar or better to normal word embeddings on the common term-based evaluation tasks. \\
One motivation for incorporating the graph-based methods to enhance the performance was because of the additional information that such graphs contain. Most word embedding methods consider a fixed sized window of few words to determine if two terms are related to each other. Named entities, however, have more complex relational structures. An actor and an organization can be related even if they are mentioned several sentences apart. An example text is shown in Figure~\ref{fig:article_entities}, where possible relations between  two actor entities, \emph{``Donald Trump"} and \emph{``Fred C. Trump"} is shown. If the context window is limited to a few words around the center word, then the relationship would remain detected, or else a window size of over $53$ tokens is required. On the other hand, if we look at the sentence distance of even one sentence away,  \emph{``Donald Trump"} and \emph{``Fred C. Trump"} would be considered related. This is one of the reasons behind the choice of LOAD model as a co-occurrence network. The LOAD model applies a sentence-based window to find entity relations. In following sections, we first describe the word embedding models for raw text and describe how to go from the raw text to a corpus with named entity annotation. Finally, we show how to use graph-based methods to obtain entity embeddings from a co-occurrence graph. 

\begin{figure}
\centering 
\resizebox{0.90\textwidth}{0.2\textwidth}{      
\input{images/article_entities.tex}
}
\caption{An example text to show the relation between two actor entities, \emph{``Donald Trump"} and \emph{``Fred C. Trump"}. If the word-based window size is applied then the relationships between the two entities is not captured, even though they are only one sentence apart. }
\label{fig:article_entities}
\end{figure}
\section{Word Embeddings on Raw Text}\label{sec:raw}
State-of-the-art word embedding models are trained on raw text (without entity annotation). The word2vec and GloVe models, explained in Chapter~\ref{chap:background}, Sections~\ref{subsec:word2vec} and~\ref{subsec:GloVe} respectively, are chosen as representative of classical word embeddings.From the word2vec model, the skip-gram architecture is chosen. We refer to the word2vec and GloVe model as $rW2V$ and  $rGLV$, where \emph{``r"} denotes raw text input.The text is cleaned only by common pre-processing steps. The corpus is tokenized into words and stop words and punctuations are removed. Non-alphabetic characters, like Chinese or Arabic vocabulary, are disregard and multiple white spaces are reduced to only one.  Any HTML tags or numeric values are also removed from the text, to produce a clean document, containing only terms relevant to the model. In addition, all the words are converted to their lower-case form to avoid multiple representations of a single term. Since no entities are annotated, all words are treated as terms. 
\section{Word Embeddings on Annotated Text}\label{sec:annotated}
In Chapter~\ref{chap:intro}, we defined named entities $N$ as a subset of all terms $N\subseteq T$ that fall into a pre-defined category. In other words, terms that have a specific type are considered as named entities. Hence, the simplest way to obtain their embeddings would be to run the well-established word embedding models on the annotated text. Our contribution to these models is not the modification of the underlying method, but the input. The input of the model is no longer the raw text, but a corpus with annotated named entities. The task of annotation involves POS tagging, named entity recognition and disambiguation. Since word embeddings learn the representation for each token in the text, our definition of the token is changed. A token is no longer a single word separated by space but named entity with a specific type. Any word that does not fit into any of the pre-defined types is considered as a term. In the set $T$ multiple entities might share ambiguous labels, but after annotation they can be represented with a unique identifier. In our case, the identifier is the type of the word and numeric key, e.g,  \emph{ACT\_17}, for an actor. As a result, compound words, such as names of people and organizations are considered as one word. All mentions of  \emph{A\_17}, which corresponds to the actor  \emph{``Barack Obama"} are represented with a single notation. Thus, making the text less ambiguous. In addition, depending on the performance of the named entity recognition tool all other indirect mentions, e.g. \emph{``Mr. President"}, \emph{``President of the United States"}, are also replaced with their corresponding entity identifier. The same applies to different mentions of dates that result into date embeddings. Words that belong to more than one entity type, have an embedding for each type. For example, \emph{``Washington"} can either be a name of a person or a city and will have two separate embeddings for the actor and location context. The same strategy applies for terms, e.g,  \emph{TER\_22}.  Adding the type information in the name helps us identify and group named entities easier. \\
\begin{figure}
\centering 
\resizebox{0.85\textwidth}{0.28\textwidth}{      
\input{images/annotation.tex}
}
\caption{An example of the pre-processing for annotated text. All the entity mentions are replaced with their unique identifier and the unnecessary tags POS-tags are removed. }
\label{fig:annotation}
\end{figure}
Since entity annotation requires POS tagging, we used the POS tags to remove stop-word, punctuation. After annotation all the entities are replaced with their unique identifier and remaining words are considered as terms ($T\setminus  N$). An example of the text after annotation and pre-processing is shown in Figure~\ref{fig:annotation}. After annotation, we use word2vec and GloVe and refer to them as $aW2V$ and $aGLV$, where \emph{``a"} denotes the use of annotated text. 
%
%For text pre-processing, punctuation and stop-word are removed and terms in the text are filtered to a certain POS-tag. Unnecessary tags are removed, namely  wh-determiner, wh-pronouns, wh-adverbs, common verbs, such as  \emph{have} and  \emph{do} in past or present, interjections (e.g. ah!, dear me!) and other terms that mostly fall into stop-words category such as: predeterminer (both and a lot of), possessive endings and prepositions (until, before,...), determiner (a, the, every) and coordinating conjunction (and, but, or). 

\section{Node Embeddings of Cooccurrence Graphs}\label{sec:graph_based}
We defined a co-occurrence graph of words in Chapter~\ref{chap:background}, as a graph $G=(T,E)$, where $T$ is the set of terms in the vocabulary as nodes and $E$ set of edges and the edge weights encode some notion of distance between the words. We extract such graph from the textual corpus with named-entity annotations. As a result, we obtain a weighted heterogeneous graph $G=(N,E)$ of named entity, where the set $N$ contains entities of different types and terms. For entity annotations in particular, implicit networks can serve as graph representations. By using stenece-window for finding the relationship between entities, they can capture entity relations even stentences apart. ~\brackettext{\cite{DBLP:conf/sigir/SpitzG16}}. Thus, by embedding the nodes, we can obtain word embeddings for both terms and entities.  We later show that this method achieves a better performance in comparison to $W2Vr$ and $GLVr$ on the annotated text. For the node embeddings, we chose DeepWalk ($DW$) and VERSE ($VRS$), explained in Chapter~\ref{chap:background}, Sections~\ref{subsec:DeepWalk} and ~\ref{subsec:VERSE} respectively. However, since we work on weighted graphs and weights display the strength of the relationships, weighted random walks are introduced to replace the uniform random walk in the DeepWalk model. The probability of a node visiting its neighbors is proportional to the edge weight. As a result, the probability of visiting node $j$ from vertex $i$ with edge weight $e_{i,j}$,  is given in Equation~\ref{eq:edge_weight}, where $ E_{ i }$ denotes the set of all edges starting from node $t_{i}$. The stronger the relationship, the more probable is it to be visited. 
\begin{equation}
P_{i,j}=\frac{e_{i,j}}{\sum _{ e_{ k }\in E_{ i } }^{  }{ e_{k} } }
\label{eq:edge_weight}
\end{equation}
In a word co-occurrence
graph, some words co-occur many times and some only a few times. Therefore, the weights can fluctuate between small numbers up to tens of thousands, resulting in highly unbalanced transition probabilities, causing some nodes to never be visited in a random walk. To solve this issue and create a more balanced weight distribution, $ \log$ mapping is used. In the $\log$ mapping, all edge weights are replaced by their logarithm and the transition probabilities are computed accordingly. We refer to the DeepWalk with $log$ mapping as ($DW_{log}$) and without $log$ mapping as ($DW_{id}$). Because of the unbalanced weight distribution, the optimization objective for the second-order proximity of LINE model becomes ill-defined. Hence, one part of the embedding for this model cannot be trained properly and since only half an embedding does not represent the model we did not use it for generating entity embeddings. \\
\begin{figure}
\centering 
\resizebox{0.50\textwidth}{0.35\textwidth}{      
\input{images/simrank.tex}
}
\caption{An example of SimRank similarity. In this graph, although the relationship between the president of Iran and the US is discovered by SimRank, the more important relationships, namely direct neighbors are disregarded. The similarity between \emph{``Barack Obama"} and the direct neighbors are zero.  }
\label{fig:simrank}
\end{figure}
The VERSE model does not support the incorporation of the edge weights. From the three similarity measure that VERSE offers, we chose Adjacency Similarity to produce the entity embeddings. Our reason is two-folded: first, the PPR relates to the stationary distribution of a random walk with restart and we already represented the random walk-based models with DeepWalk. Second, SimRank is a measure of structural relatedness, between two nodes, based on the assumption that two nodes are
similar if they are connected to other similar nodes. Although words that happen often with the same words can themselves be considered similar, if we only look at nodes that have common neighbors we lose most of the adjacency relations in the graph. An example is illustrated in Figure~\ref{fig:simrank}. Since the SimRank of two nodes with no common neighbor, is zero, the direct neighbors are disregarded. However, SimRank tends to discover interesting relationship, such as the one shown in Figure~\ref{fig:simrank} and possibly if a new similarity measure is introduced that creates a trade-off between SimRank and Adjacency Similarity, the results might improve. For this study, however, we choose only Adjacency Similarity to learn the embeddings. Adjacency Similarity is based on the normalized adjacency matrix and captures the neighborhood relations. Moreover, it correlates with the first-order proximity in LINE and can be used as representative of first-order proximity. \\
\section{Similarity Between Embeddings }\label{sec:similarity}
The dense word vector is learned to capture the semantics of the words and hence similar words are close in the induced space. \emph{cosine similarity} helps to capture this semantic closeness and is the most common similarity measure for the word vector. The cosine similarity is a measure that calculates the cosine of the angle between two vectors. This metric is a measurement of orientation and not magnitude and it is derived from the equation of a dot product between two vectors (Equation~\ref{eq:cosine}). The vectors are normalized by their length, which removes the influence of their magnitude on the similarity. The norm of the vector is somewhat related to the overall frequency of which words occur in the training corpus, but the direction is unaffected by this. So in order for a common word like \emph{``frog"} to still be similar to a less frequent word like \emph{``Anura"} (a type of frog), cosine distance which only looks at the direction works better than simple Euclidean distance. Moreover, cosine similarity is symmetric and changing the order of vectors in the dot product does not effect the final result.
\begin{equation}
\begin{split}
\overrightarrow { w_a } .\overrightarrow { w_b } =\parallel \overrightarrow { w_a } \parallel \parallel \overrightarrow { w_b } \parallel cos\theta 
\\
cos\theta =\frac { \overrightarrow { w_a } .\overrightarrow { w_b }  }{ \parallel \overrightarrow { w_a } \parallel \parallel \overrightarrow { w_b } \parallel  } 
\end{split}
\label{eq:cosine}
\end{equation}
\noindent
In this chapter,  two methods to extract entity and term embeddings from an annotated corpus were introduced. One method learns the embeddings directly from the text, while the other one generates a co-occurrence graph and embeds the nodes. In the next Chapter we attempt to create embeddings separable by entity type and in In Chapter~\ref{chap:eval}, we report the evaluation results for both entity and facetted embeddings. We also show why the extraction of a co-occurrence graph is crucial for achieving better performance. 