\begin{center}
  \textsc{Abstract}
\end{center}
%
\noindent
%
Word embeddings are a low-dimensional vector representation of words, which map the words of a vocabulary to vectors of real numbers in a low-dimensional space, such that words with similar meaning are mapped to nearby points. These dense vector representations are used as features in many information retrieval and language processing tasks. However, while many such tasks involve or even rely on named entities, popular word embedding models so far fail to acknowledge them as a unique identity and treat them as terms. Named entities are real-world objects, such as actors and locations, that can be denoted with a proper name and fall into pre-defined categories. Creating a meaningful vector representation of named entities is by no means a trivial task and while it seems intuitive that annotating named entities in the training corpus should result in more intelligent word features, naively applying word embedding techniques to an annotated corpus degrades the performance in comparison to embeddings trained on raw, unannotated text. Moreover, annotating the corpus adds more complexity to data, since the word is no longer just terms but have a specific type (e.g., location or actor). In an annotated corpus, where the type of each word is known, studying the different named entity types separately and investigating their impact on word semantics is also a possibility. \\
In this thesis, we investigate approaches to jointly train word and entity embeddings on a large corpus with automatically annotated and linked entities. Furthermore, we extend our models to capture the relation of a word to a specific type of entities in separate components, to create a more interpretable representation and shine some light in the significance of entity types for word semantics. For training word and entity embeddings, we discuss two approaches, namely training of state-of-the-art word embeddings techniques on annotated text, as well as embedding the nodes of a co-occurrence graph representation of an annotated corpus. We take advantage of this graph structure to introduce the embeddings with separable components, where each component captures the relation of the word to named entities of a specific type. To achieve these separable components, we use the co-occurrence graph of an annotated corpus as input and apply modifications of well-established word embedding and graph embedding techniques to divide the embedding space into types of the named entities present in the text. \\
We compare the performance of all our models against the classical word embeddings trained on the raw, unannotated text on a variety of word similarity, analogy, and clustering evaluation tasks. We further explore the possible benefits and use-cases of such models on entity-specific experimental analysis. We find that simply training popular word embedding models on an annotated corpus is not enough to achieve an acceptable performance and discuss how and when node embeddings of the co-occurrence graph can restore the performance. We also conclude that dividing the embedding space into separate components, requires a more complex definition of a word's context and simple approaches are not sufficient to compete with normal word embedding on common evaluation tasks. The study of the separate component suggests that the terms in a text are the most important learning input and removing them or restricting the algorithms to other types degrades the performance on common tasks.