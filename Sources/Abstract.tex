\begin{center}
  \textsc{Abstract}
\end{center}
%
\noindent
%
Because of the technological advances of recent years and vast amount of textual content being released every minute, natural language processing has grown into an exciting area of scientific research, with the goal of learning and understanding human language content. Since a \emph{``word''} is single distinct meaningful element of speech or sentence, capturing its semantics plays an important rule for understanding human language. Word embeddings are a vector representation of words, which map the words of a high-dimensional vocabulary to vectors of real numbers in a low-dimensional space, such that words with similar meaning are mapped to nearby points. Word embeddings are used as features in many information retrieval and natural language processing tasks. However, while many such tasks involve or even rely on named entities, popular word embedding models so far fail to recognize them as a distinct concept of their own and rather treat them as terms . Although it seems intuitive that applying word embedding techniques to a corpus annotated with named entities should result in more intelligent word features, this naive approach  degrades the performance in comparison to embeddings trained on raw, unannotated text. Moreover, annotating the corpus adds more complexity to the relationship between words, different named entity types have different impact on the word semantics, which normal word embeddings fail to capture.
\\
In this thesis, we propose novel approaches to jointly train word and entity embeddings on a large corpus with automatically annotated and linked entities. Furthermore, we extend our approaches to capture the relation of a word to a specific types of entities in separate components to create a more interpretable representation and shine some light in the significance of entity types for a word semantics. We discuss two approaches for training entity embeddings, namely training of state-of-the-art word embeddings techniques on annotated text, as well as embedding the nodes of a co-occurrence graph representation of an annotated corpus. We take advantage of this graph structure to introduce the embeddings with separable components, where each component captures the relation of the word to named entities of a specific type. To achieve these separable components, we modify well-established word- and graph- embedding techniques to divide the embedding space into types of the named entities present in the text. \\
We compare the performance of our approaches against the classical word embeddings trained on the raw text on a variety of word similarity, analogy, and clustering evaluation tasks. We further explore the possible benefits and use-cases of such  models with entity-specific experimental analysis. 

%annotating the corpus adds more complexity to data, since the word is no longer just terms but have a specific type (e.g., location or actor). 
%Named entities are real-world objects, such as actors and locations, that can be denoted with a proper name and fall into pre-defined categories.
% in an annotated corpus, where the type (e.g., location or actor) of each word is known, studying the different named entity types separately and investigating their impact on word semantics is also a possibility. 

%We find that simply training popular word embedding models on an annotated corpus is not enough to achieve an acceptable performance and discuss how and when node embeddings of the co-occurrence graph can restore the performance. We also conclude that dividing the embedding space into separate components, requires a more complex definition of a word's context and simple approaches are not sufficient to compete with normal word embedding on common evaluation tasks. The study of the separate component suggests that the terms in a text are the most important learning input and removing them or restricting the algorithms to other types degrades the performance on common tasks.